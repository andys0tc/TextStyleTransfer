{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "tesis",
      "language": "python",
      "name": "tesis"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "Encoder-Decoder All.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andys0tc/TextStyleTransfer/blob/master/Encoder_Decoder_All.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4whSHbDlnQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "import nltk.data\n",
        "import glob\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchtext.data import Field, BucketIterator\n",
        "from torchtext import data\n",
        "from collections import Counter\n",
        "import spacy\n",
        "import numpy as np\n",
        "import re\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNtpJsldltg-",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "13ad0319-3915-4a9b-ca99-3743b7e54c75"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-22a0bac5-39d2-44d6-8c4a-46441f28c515\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-22a0bac5-39d2-44d6-8c4a-46441f28c515\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving testset_Hemingway.csv to testset_Hemingway.csv\n",
            "Saving trainset_Hemingway.csv to trainset_Hemingway.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWUf2nf9l8W5",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "82b1a8aa-a259-4664-b98e-7112786b8715"
      },
      "source": [
        "uploaded2 = files.upload()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-444a3523-4603-4212-a5b5-0e522fb2fe1b\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-444a3523-4603-4212-a5b5-0e522fb2fe1b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving testset_Nietzsche.csv to testset_Nietzsche.csv\n",
            "Saving trainset_Nietzsche.csv to trainset_Nietzsche.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQbbxDMjmIdf",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "8fe84078-a261-4b77-ebb3-9d9fc8b10e49"
      },
      "source": [
        "uploaded3 = files.upload()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e9afa8d0-fab2-4698-a2e9-363bd1ecbbb7\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-e9afa8d0-fab2-4698-a2e9-363bd1ecbbb7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving testset_Wilde.csv to testset_Wilde.csv\n",
            "Saving trainset_Wilde.csv to trainset_Wilde.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsiK2JDUlnQa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MIN_LEN = 5\n",
        "MAX_LEN = 30\n",
        "\n",
        "TRAIN_SIZE = 0.9\n",
        "DEV_SIZE = 0.1\n",
        "RANDOM_STATE = 101"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YStDotz0lnQd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "authors=['Hemingway','Nietzsche','Wilde']\n",
        "PATH = \"/home/julia/PycharmProjects/seq2seq/books/\"\n",
        "books=[]\n",
        "nlp = spacy.load('en')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tdd0WcVTlnQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def file_open(path):\n",
        "    #Remove special characters\n",
        "    with open(path, encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "        text = re.sub(r'([^a-záéíóúÁÉÍÓÚüA-Z0-9,.:?!¡¿ ])', '', text)\n",
        "        return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNz1ejTflnQi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_sentences(text):\n",
        "    file_doc = nlp(text)\n",
        "    sentences = list(file_doc.sents)\n",
        "    filtered_sentences=[]\n",
        "    for sent in sentences:\n",
        "        if len(sent)>=MIN_LEN:\n",
        "            filtered_sentences.append(sent)\n",
        "    print(\"Total oraciones: \",len(sentences))\n",
        "    print(\"Oraciones filtradas por longitud: \",len(filtered_sentences))\n",
        "    return filtered_sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_w-RDr9lnQl",
        "colab_type": "code",
        "colab": {},
        "outputId": "f7d6e523-6f99-4dda-a6fa-ce4e869f27a0"
      },
      "source": [
        "len(sents)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'sents' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-d81f36919a04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'sents' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdRJUOATlnQo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sents_by_author=[]\n",
        "for author in authors:\n",
        "    name_file = PATH + author\n",
        "    print(name_file)\n",
        "    books_author = []\n",
        "    sents = []\n",
        "    for file in glob.glob(name_file + \"/*.txt\"):\n",
        "        books_author.append(file)\n",
        "        txt = file_open(file)\n",
        "        #Las oraciones se filtran por longitud, mínima 10\n",
        "        sents = sents+convert_sentences(txt)\n",
        "    sents_by_author.append(sents)\n",
        "    #print(books_author)\n",
        "    books.append(books_author)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iw57qMLwlnQr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_frames={}\n",
        "d={}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4GeYNa6lnQt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=[\"df_{n_author}\".format(n_author=a) for a in authors]\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzt-CE9rlnQw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(authors)):\n",
        "    d[\"dataset_dict_{n_author}\".format(n_author=authors[i])] = {'input': [sent for sent in sents_by_author[i]], 'output': [sent for sent in sents_by_author[i]]}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylO_0xCJlnQy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_Hemingway= pd.DataFrame(d[\"dataset_dict_Hemingway\"],columns=[\"input\", 'output'])\n",
        "df_Nietzsche= pd.DataFrame(d[\"dataset_dict_Nietzsche\"],columns=[\"input\", 'output'])\n",
        "df_Wilde = pd.DataFrame(d[\"dataset_dict_Wilde\"],columns=[\"input\", 'output'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DovLH6pMlnQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_frames = {df[0]:df_Hemingway,df[1]:df_Nietzsche,df[2]:df_Wilde}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erEa6JeGlnQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(authors)):\n",
        "    train_df, test_df = train_test_split(data_frames[df[i]],\n",
        "                                         test_size=DEV_SIZE,\n",
        "                                         train_size=TRAIN_SIZE,\n",
        "                                         random_state=69,\n",
        "                                         shuffle=True)\n",
        "    train_df.to_csv(PATH + \"/\" + authors[i] + \"/datasets/trainset_\"+authors[i]+\".csv\", index=False)\n",
        "    test_df.to_csv(PATH + \"/\" + authors[i] + \"/datasets/testset_\"+authors[i]+\".csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gc41hhglnQ6",
        "colab_type": "text"
      },
      "source": [
        "# Preparación de datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXSqETh5lnQ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_output(text):\n",
        "    return [tok.text for tok in nlp.tokenizer(text)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQB0943QlnRA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_input(text):\n",
        "    return [tok.text for tok in nlp.tokenizer(text)][::-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYFd3M0flnRC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e962cf7a-1058-4385-f28c-d624789d4e10"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "\n",
        "#Fields\n",
        "SRC = Field(tokenize = tokenize_input,\n",
        "            init_token = '<sos>',\n",
        "            eos_token = '<eos>',\n",
        "            dtype = torch.long,\n",
        "            lower = True,\n",
        "            include_lengths = False,\n",
        "            batch_first = False)\n",
        "\n",
        "TRG_H = TRG_N = TRG_W = Field(tokenize = tokenize_output,\n",
        "            init_token = '<sos>',\n",
        "            eos_token = '<eos>',\n",
        "            dtype = torch.long,\n",
        "            lower = True,\n",
        "            include_lengths = False,\n",
        "            batch_first = False)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pk33rOYMlnRF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fields_h = [('src', SRC), ('trg_h', TRG_H)]\n",
        "fields_n = [('src', SRC), ('trg_n', TRG_N)]\n",
        "fields_w = [('src', SRC), ('trg_w', TRG_W)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xD0gK0JmlnRH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train_set_H, test_set_H = data.TabularDataset.splits(path=PATH+'/'+authors[0]+'/datasets/',\n",
        "train_set_H, test_set_H = data.TabularDataset.splits(path='',                                                     \n",
        "                                                train = 'trainset_'+authors[0]+'.csv',\n",
        "                                                validation = 'testset_'+authors[0]+'.csv',\n",
        "                                                format='csv',\n",
        "                                                fields=fields_h,\n",
        "                                                skip_header=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0cWlCqflnRK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train_set_N, test_set_N = data.TabularDataset.splits(path=PATH+'/'+authors[1]+'/datasets/',\n",
        "train_set_N, test_set_N = data.TabularDataset.splits(path='',\n",
        "                                                train = 'trainset_'+authors[1]+'.csv',\n",
        "                                                validation = 'testset_'+authors[1]+'.csv',\n",
        "                                                format='csv',\n",
        "                                                fields=fields_n,\n",
        "                                                skip_header=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDQ_wk_ylnRM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train_set_W, test_set_W = data.TabularDataset.splits(path=PATH+'/'+authors[2]+'/datasets/',\n",
        "train_set_W, test_set_W = data.TabularDataset.splits(path='',                                                     \n",
        "                                                train = 'trainset_'+authors[2]+'.csv',\n",
        "                                                validation = 'testset_'+authors[2]+'.csv',\n",
        "                                                format='csv',\n",
        "                                                fields=fields_w,\n",
        "                                                skip_header=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sprXHDhlnRO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "b84080f2-d108-4d5e-93a7-01f4f84aadfc"
      },
      "source": [
        "print(f'\\t Hemingway: len(train_set): {len(train_set_H)}')\n",
        "print(f'\\t Hemingway: len(dev_set): {len(test_set_H)}')\n",
        "print(f'\\t Nietzsche: len(train_set): {len(train_set_N)}')\n",
        "print(f'\\t Nietzsche: len(dev_set): {len(test_set_N)}')\n",
        "print(f'\\t Wilde: len(train_set): {len(train_set_W)}')\n",
        "print(f'\\t Wilde: len(dev_set): {len(test_set_W)}')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t Hemingway: len(train_set): 28725\n",
            "\t Hemingway: len(dev_set): 3192\n",
            "\t Nietzsche: len(train_set): 14451\n",
            "\t Nietzsche: len(dev_set): 1606\n",
            "\t Wilde: len(train_set): 15687\n",
            "\t Wilde: len(dev_set): 1743\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVlbVpXdlnRQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "22a825aa-b213-4633-e054-6651d504306e"
      },
      "source": [
        "#Solo un encoder\n",
        "SRC.build_vocab(train_set_H, train_set_N, train_set_W,\n",
        "                         #dev_set,\n",
        "                         max_size = None,\n",
        "                         min_freq = 2,\n",
        "                         vectors = \"glove.6B.300d\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:33, 2.19MB/s]                          \n",
            "100%|█████████▉| 399909/400000 [00:38<00:00, 11222.71it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_dFHpF6lnRS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRG_H.build_vocab(train_set_H,\n",
        "                         #dev_set,\n",
        "                         max_size = None,\n",
        "                         min_freq = 2,\n",
        "                         vectors = \"glove.6B.300d\")\n",
        "TRG_N.build_vocab(train_set_N,\n",
        "                         #dev_set,\n",
        "                         max_size = None,\n",
        "                         min_freq = 2,\n",
        "                         vectors = \"glove.6B.300d\")\n",
        "TRG_W.build_vocab(train_set_W,\n",
        "                         #dev_set,\n",
        "                         max_size = None,\n",
        "                         min_freq = 2,\n",
        "                         vectors = \"glove.6B.300d\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJPhsyCdlnRU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "a64e778c-1637-4de6-d756-6bfcf673df1d"
      },
      "source": [
        "print(\"Vocabulary Encoder Matrix: \",SRC.vocab.vectors.shape)\n",
        "print(\"Vocabulary Decoder Hemingway Matrix: \",TRG_H.vocab.vectors.shape)\n",
        "print(\"Vocabulary Decoder Netzsche Matrix: \",TRG_N.vocab.vectors.shape)\n",
        "print(\"Vocabulary Decoder Wilde Matrix: \",TRG_W.vocab.vectors.shape)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary Encoder Matrix:  torch.Size([13726, 300])\n",
            "Vocabulary Decoder Hemingway Matrix:  torch.Size([5868, 300])\n",
            "Vocabulary Decoder Netzsche Matrix:  torch.Size([5868, 300])\n",
            "Vocabulary Decoder Wilde Matrix:  torch.Size([5868, 300])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LF6L0OhNlnRW",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Creación del modelo "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_t8H-iflnRX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, emb_mat, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.emb_dim = emb_dim\n",
        "        self.emb_mat = emb_mat\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.embedding = nn.Embedding(self.input_dim, self.emb_dim)\n",
        "        self.embedding.weight.data.copy_(emb_mat) #matrix precargada\n",
        "        \n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, batch_first = False, dropout=dropout)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src = [src len, batch size]\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "\n",
        "        # embedded = [src len, batch size, emb dim]\n",
        "\n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "\n",
        "        # outputs = [src len, batch size, hid dim * n directions]\n",
        "        # hidden = [n layers * n directions, batch size, hid dim]\n",
        "        # cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        # outputs are always from the top hidden layer\n",
        "\n",
        "        return outputs, (hidden, cell)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeI70hc2lnRa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim,emb_mat, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.emb_dim = emb_dim\n",
        "        self.emb_mat = emb_mat\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.embedding.weight.data.copy_(emb_mat)  # load pretrained embedding matrix\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers,batch_first = False, dropout=dropout)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, input, state):\n",
        "        (hidden, cell) = state\n",
        "        # input = [batch size]\n",
        "        # hidden = [n layers * n directions, batch size, hid dim]\n",
        "        # cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        # n directions in the decoder will both always be 1, therefore:\n",
        "        # hidden = [n layers, batch size, hid dim]\n",
        "        # context = [n layers, batch size, hid dim]\n",
        "\n",
        "        #input = input.unsqueeze(0)\n",
        "\n",
        "        # input = [1, batch size]\n",
        "\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "\n",
        "        # embedded = [1, batch size, emb dim]\n",
        "\n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "\n",
        "        # output = [seq len, batch size, hid dim * n directions]\n",
        "        # hidden = [n layers * n directions, batch size, hid dim]\n",
        "        # cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        # seq len and n directions will always be 1 in the decoder, therefore:\n",
        "        # output = [1, batch size, hid dim]\n",
        "        # hidden = [n layers, batch size, hid dim]\n",
        "        # cell = [n layers, batch size, hid dim]\n",
        "        output = self.dropout(output)\n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "\n",
        "        # prediction = [batch size, output dim]\n",
        "\n",
        "        return prediction, (hidden, cell)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CC410MeBlnRd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoders):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder_H = decoders[0]\n",
        "        self.decoder_N = decoders[1]\n",
        "        self.decoder_W = decoders[2]\n",
        "\n",
        "        \n",
        "    def forward(self, src, trg, index_author,teacher_forcing_ratio):\n",
        "        # src = [src len, batch size]\n",
        "        # trg = [trg len, batch size]\n",
        "        # teacher_forcing_ratio is probability to use teacher forcing\n",
        "        # e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
        "\n",
        "        if index_author == 0:            \n",
        "            decoder = self.decoder_H\n",
        "        elif index_author == 1:\n",
        "            decoder = self.decoder_N\n",
        "        elif index_author == 2:\n",
        "            decoder = self.decoder_W\n",
        "        \n",
        "        \n",
        "        \n",
        "        #batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = decoder.output_dim\n",
        "\n",
        "        # tensor to store decoder outputs\n",
        "        #outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "        # last hidden state of the encoder is used as the initial hidden state of the decoder\n",
        "        output,(hidden, cell) = self.encoder(src)\n",
        "        \n",
        "        batch_output = F.one_hot(trg[0:1], num_classes=trg_vocab_size).float()\n",
        "\n",
        "        # first input to the decoder is the <sos> tokens\n",
        "        #input = trg[0, :]\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "            # insert input token embedding, previous hidden and previous cell states\n",
        "            if t == 1:\n",
        "                trg_t  = trg[t-1:t] #ie: <sos>\n",
        "            else:\n",
        "                if torch.rand(1) < teacher_forcing_ratio:\n",
        "                    trg_t = trg[t-1:t]\n",
        "                else:\n",
        "                        # output_t: [trg_len=1, batch_size, trg_vocab_size]\n",
        "                    trg_t = output_t.argmax(2)\n",
        "            \n",
        "            \n",
        "            # receive output tensor (predictions) and new hidden and cell states\n",
        "            output_t, (hidden, cell) = decoder(trg_t, (hidden, cell))\n",
        "            \n",
        "            output_t = output_t.unsqueeze(0)\n",
        "                # output_t: [trg_len=1, batch_size, trg_vocab_size]\n",
        "            \n",
        "            batch_output = torch.cat((batch_output, output_t), 0)\n",
        "            \n",
        "            # place predictions in a tensor holding predictions for each token\n",
        "            #outputs[t] = output\n",
        "\n",
        "            # decide if we are going to use teacher forcing or not\n",
        "            #teacher_force = random.random() < teacher_forcing_ratio\n",
        "\n",
        "            # get the highest predicted token from our predictions\n",
        "            #top1 = output.argmax(1)\n",
        "\n",
        "            # if teacher forcing, use actual next token as next input\n",
        "            # if not, use predicted token\n",
        "            #input = trg[t] if teacher_force else top1\n",
        "\n",
        "        return batch_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbmgPI6klnRf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Encoder\n",
        "INPUT_DIM = len(SRC.vocab)\n",
        "ENC_EMB_DIM =  SRC.vocab.vectors.shape[1]\n",
        "ENC_EMB_MAT = SRC.vocab.vectors\n",
        "ENC_HID_DIM = 512\n",
        "ENC_N_LAYERS = 4\n",
        "ENC_DROPOUT = 0.5\n",
        "\n",
        "#decoders General\n",
        "DEC_EMB_DIM = 256\n",
        "DEC_HID_DIM = 512\n",
        "DEC_N_LAYERS = 4\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "#Decoder Hemingway\n",
        "OUTPUT_DIMS_H = len(TRG_H.vocab)\n",
        "DEC_EMB_DIM_H = TRG_H.vocab.vectors.shape[1]\n",
        "DEC_EMB_MAT_H = TRG_H.vocab.vectors\n",
        "\n",
        "#Decoder Nietzsche\n",
        "OUTPUT_DIMS_N = len(TRG_N.vocab)\n",
        "DEC_EMB_DIM_N = TRG_N.vocab.vectors.shape[1]\n",
        "DEC_EMB_MAT_N = TRG_N.vocab.vectors\n",
        "\n",
        "#Decoder Wilde\n",
        "OUTPUT_DIMS_W = len(TRG_W.vocab)\n",
        "DEC_EMB_DIM_W = TRG_W.vocab.vectors.shape[1]\n",
        "DEC_EMB_MAT_W = TRG_W.vocab.vectors\n",
        "\n",
        "TRG_OUTPUT_DIM = [OUTPUT_DIMS_H, OUTPUT_DIMS_N, OUTPUT_DIMS_W]\n",
        "DEC_EMB_DIM = [DEC_EMB_DIM_H, DEC_EMB_DIM_N, DEC_EMB_DIM_W]\n",
        "DEC_EMB_MAT = [DEC_EMB_MAT_H, DEC_EMB_MAT_N, DEC_EMB_MAT_W]\n",
        "\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "LEARNING_RATE = 0.001\n",
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "p = 1.0 \n",
        "min_prob = 0.01\n",
        "linear_decay = 0.00125\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw-YR0gnlnRh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0dd48a51-105c-4aee-a882-2ec23a1f34e0"
      },
      "source": [
        "device"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BC0UDRN2lnRk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_iterator_H, test_iterator_H = BucketIterator.splits(\n",
        "                                    (train_set_H,  test_set_H),\n",
        "                                    batch_size = BATCH_SIZE,\n",
        "                                    sort_key = lambda x: len(x.src),\n",
        "                                    device = device,\n",
        "                                    shuffle = True)\n",
        "\n",
        "train_iterator_N, test_iterator_N = BucketIterator.splits(\n",
        "                                    (train_set_N,  test_set_N),\n",
        "                                    batch_size = BATCH_SIZE,\n",
        "                                    sort_key = lambda x: len(x.src),\n",
        "                                    device = device,\n",
        "                                    shuffle = True)\n",
        "\n",
        "train_iterator_W, test_iterator_W = BucketIterator.splits(\n",
        "                                    (train_set_W,  test_set_W),\n",
        "                                    batch_size = BATCH_SIZE,\n",
        "                                    sort_key = lambda x: len(x.src),\n",
        "                                    device = device,\n",
        "                                    shuffle = True)\n",
        "train_iterator=[train_iterator_H,train_iterator_N,train_iterator_W]\n",
        "test_iterator=[test_iterator_H, test_iterator_N, test_iterator_W]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qbz37cf5lnRm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inicialize_model(input_dim,input_emb_dim, input_emb_mat, input_hid_dim, input_n_layers, input_dropout, output_dim,output_emb_dim, output_emb_mat, output_hid_dim, output_n_layers, output_dropout):\n",
        "    enc = Encoder(input_dim=input_dim, \n",
        "                  emb_dim=input_emb_dim, \n",
        "                  emb_mat=input_emb_mat, \n",
        "                  hid_dim=input_hid_dim, \n",
        "                  n_layers=input_n_layers, \n",
        "                  dropout=input_dropout)\n",
        "                     \n",
        "    dec_h = Decoder(output_dim = output_dim[0],\n",
        "                    emb_dim = output_emb_dim[0], \n",
        "                    emb_mat = output_emb_mat[0], \n",
        "                    hid_dim = output_hid_dim, \n",
        "                    n_layers = output_n_layers, \n",
        "                    dropout = output_dropout)\n",
        "    \n",
        "    dec_n = Decoder(output_dim = output_dim[1],\n",
        "                    emb_dim = output_emb_dim[1], \n",
        "                    emb_mat = output_emb_mat[1], \n",
        "                    hid_dim = output_hid_dim, \n",
        "                    n_layers = output_n_layers, \n",
        "                    dropout = output_dropout)\n",
        "    \n",
        "    dec_w = Decoder(output_dim = output_dim[2],\n",
        "                    emb_dim = output_emb_dim[2], \n",
        "                    emb_mat = output_emb_mat[2], \n",
        "                    hid_dim = output_hid_dim, \n",
        "                    n_layers = output_n_layers, \n",
        "                    dropout = output_dropout)\n",
        "    \n",
        "    decoders=[dec_h,dec_n,dec_w]\n",
        "    model  = Seq2Seq(enc, decoders)\n",
        "    \n",
        "    return model\n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8w0VG7_lnRo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "74108c52-f8b1-4fc7-9eec-a5201ed2ac5f"
      },
      "source": [
        "model = inicialize_model(INPUT_DIM,\n",
        "                         ENC_EMB_DIM, \n",
        "                         ENC_EMB_MAT, \n",
        "                         ENC_HID_DIM, \n",
        "                         ENC_N_LAYERS, \n",
        "                         ENC_DROPOUT, \n",
        "                         TRG_OUTPUT_DIM,\n",
        "                         DEC_EMB_DIM, \n",
        "                         DEC_EMB_MAT, \n",
        "                         DEC_HID_DIM, \n",
        "                         DEC_N_LAYERS, \n",
        "                         DEC_DROPOUT\n",
        "                         )\n",
        "\n",
        "model = model.to(device)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r100%|█████████▉| 399909/400000 [00:50<00:00, 11222.71it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibAtgc1klnRp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9b79094d-fbb1-4458-c02c-399e781eff59"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 50,313,116 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LskvGndilnRr",
        "colab_type": "text"
      },
      "source": [
        "# Parámetros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYamAclrlnRs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(params=model.parameters(),lr = LEARNING_RATE)\n",
        "\n",
        "#decoder Hemingway\n",
        "TRG_PAD_IDX_H = TRG_H.vocab.stoi['<pad>']\n",
        "criterion_H = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX_H)\n",
        "\n",
        "#dec Niezsche\n",
        "TRG_PAD_IDX_N = TRG_N.vocab.stoi['<pad>']\n",
        "criterion_N = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX_N)\n",
        "\n",
        "#dec Wilde\n",
        "TRG_PAD_IDX_W = TRG_W.vocab.stoi['<pad>']\n",
        "criterion_W = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX_W)\n",
        "\n",
        "criterion=[criterion_H, criterion_N, criterion_W]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiT57_OzlnRu",
        "colab_type": "text"
      },
      "source": [
        "# Entrenar modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRROz6N-Bda0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-Sjkry9lnRv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator,teacher_forcing_ratio, optimizer, criterion, clip,training=True):\n",
        "    if training:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "        \n",
        "    epoch_loss = 0\n",
        "    loss_sum = 0\n",
        "    \n",
        "    author_index = [0]*len(iterator[0]) + [1]*len(iterator[1]) + [2]*len(iterator[2])\n",
        "    author_permutation = random.sample(author_index, len(author_index))\n",
        "    \n",
        "    for author in author_permutation:\n",
        "        if author==0: #Hemingway\n",
        "            batch = next(iter(iterator[0]))\n",
        "            src = batch.src\n",
        "            trg = batch.trg_h\n",
        "        elif author==1: #Niezsche\n",
        "            batch = next(iter(iterator[1]))\n",
        "            src = batch.src\n",
        "            trg = batch.trg_n\n",
        "        elif author==2: #Wilde\n",
        "            batch = next(iter(iterator[2]))\n",
        "            src = batch.src\n",
        "            trg = batch.trg_w\n",
        "        \n",
        "        src = src.to(device) \n",
        "        trg = trg.to(device) \n",
        "        output = model(src = src, \n",
        "                       trg = trg, \n",
        "                       index_author = author,\n",
        "                       teacher_forcing_ratio = teacher_forcing_ratio)    \n",
        "    \n",
        "       \n",
        "        output = output[1:]\n",
        "        trg = trg[1:]\n",
        "        \n",
        "        loss = criterion[author](output.view(-1, output.shape[2]), trg.view(-1))\n",
        "\n",
        "        if training:\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "            optimizer.step()\n",
        "\n",
        "  \n",
        "        loss_sum += loss.item()\n",
        "        #acc_sum += acc.item()\n",
        "\n",
        "    epoch_loss = loss_sum / len(author_permutation)\n",
        "    #epoch_acc = acc_sum / len(iterator)\n",
        "    return epoch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5YncN-9lnRx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "e86c64f6-6770-40fd-8999-0f79e919334e"
      },
      "source": [
        "print('\\nTraining:')\n",
        "train_loss_epoch = []\n",
        "dev_loss_epoch = []\n",
        "best_dev_loss = float('inf')\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    # Schedule teacher forcing\n",
        "    p = max(min_prob, (p - linear_decay)) \n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model = model,\n",
        "                       iterator = train_iterator,\n",
        "                       teacher_forcing_ratio = p,\n",
        "                       optimizer = optimizer,\n",
        "                       criterion =  criterion,\n",
        "                       clip = CLIP,\n",
        "                       training = True)\n",
        "    with torch.no_grad():\n",
        "        dev_loss = train(model = model,\n",
        "                       iterator = test_iterator,\n",
        "                       teacher_forcing_ratio = 0,\n",
        "                       optimizer = None,\n",
        "                       criterion =  criterion,\n",
        "                       clip = None,\n",
        "                       training = False)\n",
        "\n",
        "    train_loss_epoch.append(train_loss)\n",
        "    dev_loss_epoch.append(dev_loss)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if dev_loss < best_dev_loss:\n",
        "        best_dev_loss = dev_loss\n",
        "        torch.save(model.state_dict(), 'model_'+str(epoch)+'.pt')\n",
        "    print('-' * 89)\n",
        "    print(f'Epoch: {epoch+1:03} | Time: {epoch_mins}m {epoch_secs}s | TFprob: {p:.4f} | Train loss: {train_loss:.4f} | | Dev loss: {dev_loss:.4f}')\n",
        "print('-' * 89)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training:\n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch: 001 | Time: 16m 19s | TFprob: 0.9925 | Train loss: 3.4482 | | Dev loss: 2.6536\n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch: 002 | Time: 16m 17s | TFprob: 0.9913 | Train loss: 3.3616 | | Dev loss: 2.5719\n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch: 003 | Time: 16m 18s | TFprob: 0.9900 | Train loss: 3.2812 | | Dev loss: 2.4430\n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch: 004 | Time: 16m 21s | TFprob: 0.9888 | Train loss: 3.2243 | | Dev loss: 2.4143\n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch: 005 | Time: 16m 17s | TFprob: 0.9875 | Train loss: 3.1733 | | Dev loss: 2.2345\n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch: 006 | Time: 16m 21s | TFprob: 0.9863 | Train loss: 3.1188 | | Dev loss: 2.1413\n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch: 007 | Time: 16m 8s | TFprob: 0.9850 | Train loss: 3.0810 | | Dev loss: 2.2343\n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch: 008 | Time: 16m 3s | TFprob: 0.9838 | Train loss: 3.0342 | | Dev loss: 2.1937\n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch: 009 | Time: 16m 13s | TFprob: 0.9825 | Train loss: 3.0023 | | Dev loss: 2.0666\n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch: 010 | Time: 16m 8s | TFprob: 0.9813 | Train loss: 2.9585 | | Dev loss: 2.1555\n",
            "-----------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bA-TOzrNlnRz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}