{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk.data\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.data import Field, BucketIterator,LabelField\n",
    "from torchtext import data\n",
    "from collections import Counter\n",
    "from torch.autograd import Variable\n",
    "import spacy\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors=['Hemingway','Nietzsche','Wilde']\n",
    "authors_id=[0,1,2]\n",
    "PATH = \"books/\"\n",
    "books=[]\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "MIN_LEN = 5\n",
    "MAX_LEN = 30\n",
    "\n",
    "TRAIN_SIZE = 0.8\n",
    "DEV_SIZE = 0.2\n",
    "RANDOM_STATE = 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_open(path):\n",
    "    #Remove special characters\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        text = re.sub(r'([^a-záéíóúÁÉÍÓÚA-Z0-9,. ])', '', text)\n",
    "    f.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sentences(text):\n",
    "    file_doc = nlp(text)\n",
    "    sentences = list(file_doc.sents)\n",
    "    filtered_sentences=[]\n",
    "    for sent in sentences:\n",
    "        if len(sent)>=MIN_LEN:\n",
    "            filtered_sentences.append(sent)\n",
    "    print(\"Total oraciones: \",len(sentences))\n",
    "    print(\"Oraciones filtradas por longitud: \",len(filtered_sentences))\n",
    "    return filtered_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "books/Hemingway\n",
      "Total oraciones:  6149\n",
      "Oraciones filtradas por longitud:  5512\n",
      "Total oraciones:  19\n",
      "Oraciones filtradas por longitud:  16\n",
      "Total oraciones:  1905\n",
      "Oraciones filtradas por longitud:  1828\n",
      "Total oraciones:  7598\n",
      "Oraciones filtradas por longitud:  6545\n",
      "Total oraciones:  286\n",
      "Oraciones filtradas por longitud:  272\n",
      "Total oraciones:  755\n",
      "Oraciones filtradas por longitud:  681\n",
      "Total oraciones:  4247\n",
      "Oraciones filtradas por longitud:  3873\n",
      "Total oraciones:  3816\n",
      "Oraciones filtradas por longitud:  3423\n",
      "Total oraciones:  9976\n",
      "Oraciones filtradas por longitud:  8793\n",
      "books/Nietzsche\n",
      "Total oraciones:  1841\n",
      "Oraciones filtradas por longitud:  1556\n",
      "Total oraciones:  1136\n",
      "Oraciones filtradas por longitud:  1066\n",
      "Total oraciones:  6272\n",
      "Oraciones filtradas por longitud:  5579\n",
      "Total oraciones:  1284\n",
      "Oraciones filtradas por longitud:  1255\n",
      "Total oraciones:  1380\n",
      "Oraciones filtradas por longitud:  1288\n",
      "Total oraciones:  2130\n",
      "Oraciones filtradas por longitud:  1889\n",
      "Total oraciones:  1436\n",
      "Oraciones filtradas por longitud:  1366\n",
      "books/Wilde\n",
      "Total oraciones:  6022\n",
      "Oraciones filtradas por longitud:  5708\n",
      "Total oraciones:  4356\n",
      "Oraciones filtradas por longitud:  3134\n",
      "Total oraciones:  440\n",
      "Oraciones filtradas por longitud:  413\n",
      "Total oraciones:  934\n",
      "Oraciones filtradas por longitud:  922\n",
      "Total oraciones:  3222\n",
      "Oraciones filtradas por longitud:  2133\n",
      "Total oraciones:  776\n",
      "Oraciones filtradas por longitud:  770\n",
      "Total oraciones:  2540\n",
      "Oraciones filtradas por longitud:  1802\n",
      "Total oraciones:  2804\n",
      "Oraciones filtradas por longitud:  1809\n"
     ]
    }
   ],
   "source": [
    "sents_by_author=[]\n",
    "for author in authors:\n",
    "    name_file = PATH + author\n",
    "    print(name_file)\n",
    "    books_author = []\n",
    "    sents = []\n",
    "    for file in glob.glob(name_file + \"/*.txt\"):\n",
    "        books_author.append(file)\n",
    "        txt = file_open(file)\n",
    "        sents = sents+convert_sentences(txt)\n",
    "    sents_by_author.append(sents)\n",
    "    #print(books_author)\n",
    "    books.append(books_author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = []\n",
    "for i in range(len(authors)):\n",
    "    for sent in sents_by_author[i]:\n",
    "        if (len(sent)>=3):\n",
    "            row.append([sent,authors[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.DataFrame(row,columns=[\"text\", 'class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(IN, OUR, TIME, Chapter, I)</td>\n",
       "      <td>Hemingway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(They, started, two, hours, before, daylight, ...</td>\n",
       "      <td>Hemingway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(In, each, boat, ,, in, the, darkness, ,, so, ...</td>\n",
       "      <td>Hemingway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(The, shooter, sat, on, a, shooting, stool, fa...</td>\n",
       "      <td>Hemingway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Somewhere, ,, in, each, boat, ,, there, was, ...</td>\n",
       "      <td>Hemingway</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text      class\n",
       "0                        (IN, OUR, TIME, Chapter, I)  Hemingway\n",
       "1  (They, started, two, hours, before, daylight, ...  Hemingway\n",
       "2  (In, each, boat, ,, in, the, darkness, ,, so, ...  Hemingway\n",
       "3  (The, shooter, sat, on, a, shooting, stool, fa...  Hemingway\n",
       "4  (Somewhere, ,, in, each, boat, ,, there, was, ...  Hemingway"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61628</th>\n",
       "      <td>(My, own, one, Chasuble, .)</td>\n",
       "      <td>Wilde</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61629</th>\n",
       "      <td>(Laetitia, Embraces, her, Miss, Prism, .)</td>\n",
       "      <td>Wilde</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61630</th>\n",
       "      <td>(At, last, Lady, Bracknell, .)</td>\n",
       "      <td>Wilde</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61631</th>\n",
       "      <td>(My, nephew, ,, you, seem, to, be, displaying,...</td>\n",
       "      <td>Wilde</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61632</th>\n",
       "      <td>(On, the, contrary, ,, Aunt, Augusta, ,, I, ve...</td>\n",
       "      <td>Wilde</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  class\n",
       "61628                        (My, own, one, Chasuble, .)  Wilde\n",
       "61629          (Laetitia, Embraces, her, Miss, Prism, .)  Wilde\n",
       "61630                     (At, last, Lady, Bracknell, .)  Wilde\n",
       "61631  (My, nephew, ,, you, seem, to, be, displaying,...  Wilde\n",
       "61632  (On, the, contrary, ,, Aunt, Augusta, ,, I, ve...  Wilde"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, dev_df = train_test_split(df,\n",
    "                                         test_size=DEV_SIZE,\n",
    "                                         train_size=TRAIN_SIZE,\n",
    "                                         random_state=69,\n",
    "                                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"data/trainset_classification.csv\", index=False)\n",
    "dev_df.to_csv(\"data/devset_classification.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49306, 2)\n",
      "(12327, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape) \n",
    "print(dev_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2019\n",
    "\n",
    "#Torch\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "#Cuda algorithms\n",
    "torch.backends.cudnn.deterministic = True  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize='spacy',batch_first=True,include_lengths=True)\n",
    "LABEL = data.LabelField(dtype = torch.float,batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, df, fields, is_test=False, **kwargs):\n",
    "        examples = []\n",
    "        for i, row in df.iterrows():\n",
    "            label = row.target if not is_test else None\n",
    "            text = row.text\n",
    "            examples.append(data.Example.fromlist([text, label], fields))\n",
    "\n",
    "        super().__init__(examples, fields, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def sort_key(ex):\n",
    "        return len(ex.text)\n",
    "\n",
    "    @classmethod\n",
    "    def splits(cls, fields, train_df, val_df=None, test_df=None, **kwargs):\n",
    "        train_data, val_data, test_data = (None, None, None)\n",
    "        data_field = fields\n",
    "\n",
    "        if train_df is not None:\n",
    "            train_data = cls(train_df.copy(), data_field, **kwargs)\n",
    "        if val_df is not None:\n",
    "            val_data = cls(val_df.copy(), data_field, **kwargs)\n",
    "        if test_df is not None:\n",
    "            test_data = cls(test_df.copy(), data_field, True, **kwargs)\n",
    "\n",
    "        return tuple(d for d in (train_data, val_data, test_data) if d is not None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'data/'\n",
    "train = pd.read_csv(PATH+'trainset_classification.csv')\n",
    "validation = pd.read_csv(PATH+'devset_classification.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [('text',TEXT), ('label',LABEL)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['Oh', ',', 'this', 'is', 'the', 'hostility', 'of', 'light', 'to', 'the', 'shining', 'one', 'unpityingly', 'doth', 'it', 'pursue', 'its', 'course', '.'], 'label': 'Nietzsche'}\n"
     ]
    }
   ],
   "source": [
    "#loading custom dataset\n",
    "training_data=data.TabularDataset(path = 'data/trainset_classification.csv',format = 'csv',fields = fields,skip_header = True)\n",
    "\n",
    "#print preprocessed text\n",
    "print(vars(training_data.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "train_data, valid_data = training_data.split(split_ratio=0.1, random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of TEXT vocabulary: 2836\n",
      "Size of LABEL vocabulary: 3\n",
      "[(',', 5041), ('.', 4368), ('the', 4236), ('and', 2326), ('of', 2223), ('to', 1843), ('I', 1485), ('a', 1442), ('in', 1244), ('is', 973)]\n",
      "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x7fe2b4731b70>>, {'<unk>': 0, '<pad>': 1, ',': 2, '.': 3, 'the': 4, 'and': 5, 'of': 6, 'to': 7, 'I': 8, 'a': 9, 'in': 10, 'is': 11, 'that': 12, 'it': 13, 'you': 14, 'was': 15, 'he': 16, ' ': 17, 'not': 18, 'with': 19, 'as': 20, 'his': 21, 'for': 22, 'said': 23, 'have': 24, 'at': 25, 'all': 26, 'The': 27, 'be': 28, 'on': 29, 'me': 30, 'had': 31, 'do': 32, 'nt': 33, 'are': 34, 'him': 35, 'my': 36, 'one': 37, 'which': 38, 'He': 39, 'they': 40, 'this': 41, 'from': 42, 'were': 43, 'so': 44, 'we': 45, 'by': 46, 'It': 47, 'them': 48, 'but': 49, 'there': 50, 'would': 51, 'will': 52, 'out': 53, 'has': 54, 'man': 55, 'an': 56, 'up': 57, 'her': 58, 'like': 59, 'But': 60, 'what': 61, 'You': 62, 'when': 63, 'who': 64, 'its': 65, 'into': 66, 'or': 67, 'been': 68, 'more': 69, 'their': 70, 'about': 71, 'only': 72, 'very': 73, 'know': 74, 'can': 75, 'your': 76, 'no': 77, 'now': 78, 'And': 79, 'good': 80, 'could': 81, 'if': 82, 'must': 83, 'did': 84, 'down': 85, 'am': 86, 'she': 87, 'than': 88, 'see': 89, 'then': 90, 'They': 91, 'What': 92, 'time': 93, 'too': 94, 'life': 95, 'say': 96, 'any': 97, 'We': 98, 'us': 99, 'go': 100, 'some': 101, 'There': 102, 'much': 103, 'over': 104, 'never': 105, 'even': 106, 'people': 107, 'great': 108, 'back': 109, 'other': 110, 'think': 111, 'went': 112, 'also': 113, 'come': 114, 'long': 115, 'most': 116, 'That': 117, 'here': 118, 'little': 119, 'our': 120, 'how': 121, 'old': 122, 'always': 123, 'himself': 124, 'love': 125, 'still': 126, 'things': 127, 'own': 128, 'world': 129, 'should': 130, 'just': 131, 'get': 132, 'such': 133, 'In': 134, 'going': 135, 'men': 136, 's': 137, 'She': 138, 'two': 139, 'came': 140, 'every': 141, 'made': 142, 'first': 143, 'thought': 144, 'through': 145, 'way': 146, 'A': 147, 'again': 148, 'before': 149, 'want': 150, 'right': 151, 'away': 152, 'hand': 153, 'looked': 154, 'thou': 155, 'Do': 156, 'after': 157, 'really': 158, 'something': 159, 'same': 160, 'thing': 161, 'well': 162, 'head': 163, 'ever': 164, 'Colonel': 165, 'Oh': 166, 'make': 167, 'off': 168, 'where': 169, 'better': 170, 'day': 171, 'got': 172, 'nothing': 173, 'once': 174, 'bad': 175, 'itself': 176, 'may': 177, 'ones': 178, 'If': 179, 'these': 180, 'put': 181, 'tell': 182, 'No': 183, 'Well': 184, 'asked': 185, 'everything': 186, 'quite': 187, 'those': 188, 'against': 189, 'hands': 190, 'room': 191, 'Then': 192, 'Lord': 193, 'because': 194, 'm': 195, 'thy': 196, 'upon': 197, '...': 198, 'Zarathustra': 199, 'does': 200, 'last': 201, 'look': 202, 'many': 203, 'night': 204, 'take': 205, 'anything': 206, 'each': 207, 'ye': 208, 'Ill': 209, 'however': 210, 'God': 211, 'This': 212, 'art': 213, 'course': 214, 'saw': 215, 'Mr.': 216, 'table': 217, 'without': 218, 'called': 219, 'eyes': 220, 're': 221, 'All': 222, 'How': 223, 'My': 224, 'When': 225, 'being': 226, 'heart': 227, 'big': 228, 'fact': 229, 'speak': 230, 'under': 231, 'To': 232, 'between': 233, 'music': 234, 'order': 235, 'perhaps': 236, 'whom': 237, 'yet': 238, 'For': 239, 'Yes': 240, 'wish': 241, 'Dorian': 242, 'means': 243, 'whole': 244, 'work': 245, 'LORD': 246, 'new': 247, 'place': 248, 'soul': 249, 'thee': 250, 'boy': 251, 'bull': 252, 'girl': 253, 'knew': 254, 'nature': 255, 'shall': 256, 'woman': 257, 'another': 258, 'give': 259, 'morning': 260, 'myself': 261, 'road': 262, 'water': 263, 'As': 264, 'face': 265, 'though': 266, 'unto': 267, 'around': 268, 'ca': 269, 'door': 270, 'light': 271, 'might': 272, 'moment': 273, 'MRS': 274, 'far': 275, 'find': 276, 'idea': 277, 'let': 278, 'sat': 279, 'told': 280, 'took': 281, 'word': 282, 'English': 283, 'Jack': 284, 'feel': 285, 'heard': 286, 'matter': 287, 'three': 288, 'while': 289, 'Brett': 290, 'One': 291, 'With': 292, 'dear': 293, 'found': 294, 'house': 295, 'looking': 296, 'read': 297, 'truth': 298, 'Why': 299, 'across': 300, 'body': 301, 'dark': 302, 'enough': 303, 'felt': 304, 'side': 305, 'spirit': 306, 'whose': 307, 'Catherine': 308, 'Now': 309, 'among': 310, 'ask': 311, 'beautiful': 312, 'behind': 313, 'end': 314, 'hath': 315, 'left': 316, 'mine': 317, 'part': 318, 'sun': 319, 'talk': 320, 'together': 321, 'turned': 322, 'war': 323, 'At': 324, 'along': 325, 'bed': 326, 'believe': 327, 'dead': 328, 'present': 329, 'themselves': 330, 've': 331, 'Bill': 332, 'CHILTERN': 333, 'Is': 334, 'Its': 335, 'Mrs.': 336, 'higher': 337, 'hope': 338, 'kind': 339, 'others': 340, 'started': 341, 'true': 342, 'become': 343, 'coming': 344, 'd': 345, 'pleasure': 346, 'question': 347, 'says': 348, 'seemed': 349, 'understand': 350, 'After': 351, 'German': 352, 'both': 353, 'brought': 354, 'drink': 355, 'feeling': 356, 'few': 357, 'general': 358, 'hear': 359, 'line': 360, 'live': 361, 'mean': 362, 'name': 363, 'sense': 364, 'stand': 365, 'wo': 366, 'His': 367, 'Lady': 368, 'So': 369, 'Where': 370, 'almost': 371, 'book': 372, 'cold': 373, 'country': 374, 'evil': 375, 'existence': 376, 'gone': 377, 'held': 378, 'mind': 379, 'often': 380, 'sleep': 381, 'small': 382, 'therefore': 383, 'walked': 384, 'O': 385, 'SIR': 386, 'answered': 387, 'best': 388, 'case': 389, 'done': 390, 'feet': 391, 'free': 392, 'friend': 393, 'greatest': 394, 'indeed': 395, 'lay': 396, 'lovely': 397, 'past': 398, 'self': 399, 'stood': 400, 'suppose': 401, 'why': 402, 'years': 403, 'Good': 404, 'LADY': 405, 'Of': 406, 'Robert': 407, 'call': 408, 'certain': 409, 'except': 410, 'hard': 411, 'hell': 412, 'lie': 413, 'mother': 414, 'pain': 415, 'seen': 416, 'sort': 417, 'until': 418, 'young': 419, 'Ah': 420, 'Cohn': 421, 'Let': 422, 'Not': 423, 'ROBERT': 424, 'alone': 425, 'already': 426, 'brother': 427, 'else': 428, 'fish': 429, 'form': 430, 'friends': 431, 'makes': 432, 'moral': 433, 'seems': 434, 'stay': 435, 'terrible': 436, 'try': 437, 'wine': 438, 'words': 439, 'Gray': 440, 'Here': 441, 'above': 442, 'age': 443, 'car': 444, 'die': 445, 'doth': 446, 'front': 447, 'full': 448, 'glass': 449, 'history': 450, 'home': 451, 'knowledge': 452, 'philosophy': 453, 'please': 454, 'poor': 455, 'rather': 456, 'street': 457, 'town': 458, 'whether': 459, 'wrong': 460, 'Manuel': 461, 'Or': 462, 'Thus': 463, 'WINDERMERE': 464, 'beauty': 465, 'became': 466, 'certainly': 467, 'conscience': 468, 'cut': 469, 'days': 470, 'eat': 471, 'eye': 472, 'father': 473, 'fight': 474, 'finally': 475, 'gave': 476, 'god': 477, 'hair': 478, 'heavy': 479, 'joy': 480, 'justice': 481, 'longer': 482, 'loved': 483, 'merely': 484, 'necessary': 485, 'passed': 486, 'point': 487, 'round': 488, 'strange': 489, 'sure': 490, 'taken': 491, 'use': 492, 'Christianity': 493, 'Did': 494, 'Henry': 495, 'Illingworth': 496, 'Mike': 497, 'Nietzsche': 498, 'act': 499, 'boat': 500, 'child': 501, 'culture': 502, 'doctor': 503, 'earth': 504, 'fine': 505, 'high': 506, 'leave': 507, 'less': 508, 'modern': 509, 'morality': 510, 'mouth': 511, 'need': 512, 'next': 513, 'power': 514, 'son': 515, 'suffering': 516, 'taste': 517, 'thus': 518, 'times': 519, 'today': 520, 'trees': 521, 'used': 522, 'waiting': 523, 'whatever': 524, 'wife': 525, 'women': 526, 'Christian': 527, 'account': 528, 'air': 529, 'beside': 530, 'broken': 531, 'children': 532, 'crowd': 533, 'darling': 534, 'doing': 535, 'dream': 536, 'forward': 537, 'half': 538, 'highest': 539, 'individual': 540, 'language': 541, 'law': 542, 'lot': 543, 'making': 544, 'philosopher': 545, 'profound': 546, 'real': 547, 'remember': 548, 'seem': 549, 'short': 550, 'since': 551, 'society': 552, 'talking': 553, 'towards': 554, 'voice': 555, 'white': 556, 'wind': 557, 'Gerald': 558, 'Italian': 559, 'Paris': 560, 'Will': 561, 'care': 562, 'clear': 563, 'comes': 564, 'dangerous': 565, 'future': 566, 'glad': 567, 'goes': 568, 'happy': 569, 'hold': 570, 'humanity': 571, 'ideal': 572, 'instinct': 573, 'least': 574, 'nice': 575, 'open': 576, 'person': 577, 'pity': 578, 'precisely': 579, 'pretty': 580, 'priest': 581, 'reason': 582, 'show': 583, 'strong': 584, 'tired': 585, 'toward': 586, 'wanted': 587, 'wonderful': 588, 'On': 589, 'beyond': 590, 'century': 591, 'common': 592, 'cried': 593, 'different': 594, 'evening': 595, 'fear': 596, 'further': 597, 'girls': 598, 'greater': 599, 'hot': 600, 'human': 601, 'keep': 602, 'll': 603, 'nor': 604, 'ought': 605, 'problem': 606, 'reality': 607, 'river': 608, 'set': 609, 'strength': 610, 'thinking': 611, 'tragedy': 612, 'value': 613, 'virtue': 614, 'An': 615, 'Fontan': 616, 'Germans': 617, 'Were': 618, 'Ye': 619, 'absolutely': 620, 'afraid': 621, 'afternoon': 622, 'black': 623, 'books': 624, 'bottle': 625, 'death': 626, 'doubt': 627, 'experience': 628, 'feelings': 629, 'hotel': 630, 'inside': 631, 'large': 632, 'learn': 633, 'liked': 634, 'lost': 635, 'meaning': 636, 'ourselves': 637, 'paper': 638, 'perfect': 639, 'philosophers': 640, 'picture': 641, 'possible': 642, 'public': 643, 'rich': 644, 'shook': 645, 'shot': 646, 'simply': 647, 'sir': 648, 'slowly': 649, 'straight': 650, 'subject': 651, 'train': 652, 'tried': 653, 'trying': 654, 'turn': 655, 'window': 656, 'within': 657, 'Even': 658, 'Every': 659, 'Frazer': 660, 'Gran': 661, 'Maestro': 662, 'Miss': 663, 'Nick': 664, 'Your': 665, 'although': 666, 'attack': 667, 'began': 668, 'belief': 669, 'change': 670, 'character': 671, 'close': 672, 'drunk': 673, 'exactly': 674, 'floor': 675, 'given': 676, 'happened': 677, 'having': 678, 'husband': 679, 'ideas': 680, 'impossible': 681, 'kept': 682, 'known': 683, 'knows': 684, 'leaned': 685, 'learned': 686, 'legs': 687, 'living': 688, 'lying': 689, 'mans': 690, 'married': 691, 'money': 692, 'move': 693, 'opened': 694, 'origin': 695, 'outside': 696, 'play': 697, 'portrait': 698, 'remain': 699, 'send': 700, 'shame': 701, 'sitting': 702, 'step': 703, 'sympathy': 704, 'taking': 705, 'wet': 706, 'wisdom': 707, 'wise': 708, 'American': 709, 'Dionysian': 710, 'GORING': 711, 'Have': 712, 'Madame': 713, 'Nietzsches': 714, 'THE': 715, 'Tell': 716, 'These': 717, 'Who': 718, 'Zurito': 719, 'animals': 720, 'arm': 721, 'ascetic': 722, 'bring': 723, 'carry': 724, 'chair': 725, 'count': 726, 'deep': 727, 'delight': 728, 'devil': 729, 'died': 730, 'dinner': 731, 'dreadful': 732, 'effect': 733, 'enemy': 734, 'expression': 735, 'faith': 736, 'fall': 737, 'fellow': 738, 'help': 739, 'holding': 740, 'hour': 741, 'late': 742, 'low': 743, 'mankind': 744, 'noble': 745, 'oclock': 746, 'particular': 747, 'passion': 748, 'red': 749, 'regard': 750, 'religion': 751, 'revenge': 752, 'ring': 753, 'saying': 754, 'science': 755, 'sea': 756, 'second': 757, 'seriously': 758, 'silly': 759, 'smiled': 760, 'sometimes': 761, 'sorrow': 762, 'speaking': 763, 'stairs': 764, 'standing': 765, 'state': 766, 'stronger': 767, 'ten': 768, 'thine': 769, 'tragic': 770, 'waiter': 771, 'watch': 772, 'watched': 773, 'Basil': 774, 'Cecily': 775, 'Come': 776, 'England': 777, 'Europe': 778, 'French': 779, 'Harry': 780, 'John': 781, 'Maybe': 782, 'Sir': 783, 'able': 784, 'actual': 785, 'appearance': 786, 'blood': 787, 'carried': 788, 'coffee': 789, 'conditions': 790, 'contempt': 791, 'cruel': 792, 'de': 793, 'desire': 794, 'duty': 795, 'early': 796, 'example': 797, 'five': 798, 'genius': 799, 'gentleman': 800, 'getting': 801, 'horses': 802, 'ice': 803, 'image': 804, 'important': 805, 'instincts': 806, 'interested': 807, 'interesting': 808, 'killed': 809, 'latter': 810, 'mountains': 811, 'moving': 812, 'names': 813, 'neck': 814, 'opposite': 815, 'piece': 816, 'pocket': 817, 'position': 818, 'praise': 819, 'pride': 820, 'rain': 821, 'ready': 822, 'run': 823, 'running': 824, 'sacrifice': 825, 'sad': 826, 'secret': 827, 'shadow': 828, 'shows': 829, 'sight': 830, 'silver': 831, 'six': 832, 'soon': 833, 'sound': 834, 'speaks': 835, 'square': 836, 'start': 837, 'stopped': 838, 'struggle': 839, 'sudden': 840, 'suddenly': 841, 'sweet': 842, 'thank': 843, 'thereby': 844, 'thinks': 845, 'wait': 846, 'wants': 847, 'wild': 848, 'yourself': 849, '  ': 850, 'Enter': 851, 'GERALD': 852, 'Germany': 853, 'Greek': 854, 'MY': 855, 'Our': 856, 'Rinaldi': 857, 'Schopenhauer': 858, 'Verily': 859, 'Wagner': 860, 'accept': 861, 'actually': 862, 'ahead': 863, 'allow': 864, 'becomes': 865, 'besides': 866, 'box': 867, 'built': 868, 'circumstances': 869, 'cool': 870, 'courage': 871, 'danger': 872, 'development': 873, 'error': 874, 'eternal': 875, 'fallen': 876, 'false': 877, 'family': 878, 'fast': 879, 'figure': 880, 'fire': 881, 'fond': 882, 'forms': 883, 'four': 884, 'gets': 885, 'gold': 886, 'grew': 887, 'horrible': 888, 'horror': 889, 'inner': 890, 'kinds': 891, 'later': 892, 'laughter': 893, 'letter': 894, 'listen': 895, 'marry': 896, 'mere': 897, 'namely': 898, 'nearly': 899, 'nowadays': 900, 'paid': 901, 'peoples': 902, 'period': 903, 'pulled': 904, 'putting': 905, 'remains': 906, 'rest': 907, 'scientific': 908, 'sent': 909, 'sick': 910, 'sorry': 911, 'spake': 912, 'spirits': 913, 'study': 914, 'success': 915, 'thin': 916, 'tight': 917, 'touch': 918, 'trouble': 919, 'verily': 920, 'view': 921, 'views': 922, 'walk': 923, 'written': 924, 'yes': 925, 'America': 926, 'Cheveley': 927, 'Duchess': 928, 'From': 929, 'Hans': 930, 'Just': 931, 'Like': 932, 'Miller': 933, 'Perhaps': 934, 'Please': 935, 'Tenente': 936, 'Yet': 937, 'absolute': 938, 'according': 939, 'ancient': 940, 'anybody': 941, 'apart': 942, 'appeared': 943, 'artists': 944, 'believed': 945, 'bitter': 946, 'blue': 947, 'bound': 948, 'break': 949, 'bright': 950, 'brown': 951, 'captain': 952, 'carrying': 953, 'caught': 954, 'cave': 955, 'chance': 956, 'clothes': 957, 'complete': 958, 'connection': 959, 'corner': 960, 'critic': 961, 'cruelty': 962, 'deal': 963, 'dry': 964, 'eating': 965, 'especially': 966, 'extremely': 967, 'fell': 968, 'fighting': 969, 'fill': 970, 'finds': 971, 'fingers': 972, 'force': 973, 'freedom': 974, 'frightened': 975, 'gaze': 976, 'generally': 977, 'golden': 978, 'goodness': 979, 'hardly': 980, 'hat': 981, 'heads': 982, 'herself': 983, 'hitherto': 984, 'hours': 985, 'houses': 986, 'hundred': 987, 'influence': 988, 'instance': 989, 'knife': 990, 'lack': 991, 'laid': 992, 'laugh': 993, 'lets': 994, 'lies': 995, 'lived': 996, 'lives': 997, 'major': 998, 'met': 999, 'moments': 1000, 'months': 1001, 'movement': 1002, 'mysterious': 1003, 'narrow': 1004, 'onto': 1005, 'papers': 1006, 'pay': 1007, 'placed': 1008, 'primordial': 1009, 'probably': 1010, 'produced': 1011, 'purpose': 1012, 'ran': 1013, 'rate': 1014, 'reached': 1015, 'respect': 1016, 'return': 1017, 'school': 1018, 'seat': 1019, 'sees': 1020, 'serious': 1021, 'shoulders': 1022, 'signs': 1023, 'somewhere': 1024, 'sphere': 1025, 'spring': 1026, 'stone': 1027, 'stop': 1028, 'tears': 1029, 'tendency': 1030, 'third': 1031, 'thirst': 1032, 'understood': 1033, 'uniform': 1034, 'unless': 1035, 'usually': 1036, 'virtues': 1037, 'weather': 1038, 'yourselves': 1039, '....': 1040, 'Algernon': 1041, 'Apollonian': 1042, 'Campbell': 1043, 'Can': 1044, 'Daughter': 1045, 'France': 1046, 'Hence': 1047, 'Jake': 1048, 'Lets': 1049, 'Never': 1050, 'Nothing': 1051, 'Only': 1052, 'Roman': 1053, 'Thank': 1054, 'Their': 1055, 'Thou': 1056, 'Through': 1057, 'Would': 1058, 'absurd': 1059, 'acts': 1060, 'altogether': 1061, 'appears': 1062, 'arms': 1063, 'artist': 1064, 'attained': 1065, 'awful': 1066, 'bar': 1067, 'begin': 1068, 'beginning': 1069, 'behold': 1070, 'bit': 1071, 'blew': 1072, 'blind': 1073, 'boatman': 1074, 'bottom': 1075, 'brethren': 1076, 'capable': 1077, 'changed': 1078, 'conclusion': 1079, 'contrary': 1080, 'cry': 1081, 'curious': 1082, 'divine': 1083, 'draw': 1084, 'dressed': 1085, 'dying': 1086, 'ears': 1087, 'eight': 1088, 'either': 1089, 'element': 1090, 'enter': 1091, 'essence': 1092, 'existed': 1093, 'explained': 1094, 'extent': 1095, 'faces': 1096, 'fate': 1097, 'flowers': 1098, 'fresh': 1099, 'green': 1100, 'grow': 1101, 'handed': 1102, 'happen': 1103, 'hidden': 1104, 'hit': 1105, 'honour': 1106, 'kill': 1107, 'kingdom': 1108, 'laughing': 1109, 'letters': 1110, 'lights': 1111, 'lines': 1112, 'lips': 1113, 'lose': 1114, 'loves': 1115, 'lunch': 1116, 'machine': 1117, 'manner': 1118, 'meet': 1119, 'motives': 1120, 'mud': 1121, 'nose': 1122, 'opinion': 1123, 'opportunity': 1124, 'painful': 1125, 'pale': 1126, 'passions': 1127, 'personality': 1128, 'picked': 1129, 'powerful': 1130, 'pray': 1131, 'proper': 1132, 'proud': 1133, 'quiet': 1134, 'reading': 1135, 'relations': 1136, 'resentment': 1137, 'rose': 1138, 'saith': 1139, 'sake': 1140, 'serve': 1141, 'shoot': 1142, 'shut': 1143, 'simple': 1144, 'slow': 1145, 'smell': 1146, 'smile': 1147, 'soldier': 1148, 'sore': 1149, 'sought': 1150, 'swam': 1151, 'takes': 1152, 'teach': 1153, 'telling': 1154, 'terror': 1155, 'truly': 1156, 'type': 1157, 'ugly': 1158, 'weak': 1159, 'week': 1160, 'worked': 1161, 'worse': 1162, 'worst': 1163, 'worth': 1164, 'wrote': 1165, 'youth': 1166, '2': 1167, 'Besides': 1168, 'British': 1169, 'Canterville': 1170, 'Chiltern': 1171, 'Except': 1172, 'George': 1173, 'Get': 1174, 'Go': 1175, 'Had': 1176, 'Her': 1177, 'Jews': 1178, 'Life': 1179, 'London': 1180, 'Margaret': 1181, 'New': 1182, 'Retana': 1183, 'Romero': 1184, 'San': 1185, 'Socrates': 1186, 'Such': 1187, 'Swallow': 1188, 'Walcott': 1189, 'Was': 1190, 'Worthing': 1191, 'Yea': 1192, 'abysses': 1193, 'ai': 1194, 'answer': 1195, 'ashamed': 1196, 'ass': 1197, 'ate': 1198, 'awfully': 1199, 'baby': 1200, 'badly': 1201, 'bear': 1202, 'beat': 1203, 'beer': 1204, 'below': 1205, 'bird': 1206, 'bow': 1207, 'bridge': 1208, 'broad': 1209, 'brush': 1210, 'bulls': 1211, 'café': 1212, 'canvas': 1213, 'capes': 1214, 'cause': 1215, 'circle': 1216, 'city': 1217, 'clean': 1218, 'cloud': 1219, 'concierge': 1220, 'contrast': 1221, 'conversation': 1222, 'covered': 1223, 'created': 1224, 'crossed': 1225, 'crying': 1226, 'dare': 1227, 'declare': 1228, 'deed': 1229, 'delusion': 1230, 'depth': 1231, 'destruction': 1232, 'developed': 1233, 'distance': 1234, 'doctors': 1235, 'dost': 1236, 'drank': 1237, 'drawn': 1238, 'drive': 1239, 'driver': 1240, 'earlier': 1241, 'enemies': 1242, 'entirely': 1243, 'essential': 1244, 'events': 1245, 'excessively': 1246, 'experiences': 1247, 'extraordinary': 1248, 'flat': 1249, 'fly': 1250, 'following': 1251, 'follows': 1252, 'foreign': 1253, 'forget': 1254, 'garden': 1255, 'gate': 1256, 'giving': 1257, 'glasses': 1258, 'gods': 1259, 'gospel': 1260, 'gray': 1261, 'ground': 1262, 'growing': 1263, 'grown': 1264, 'happiness': 1265, 'hast': 1266, 'hate': 1267, 'height': 1268, 'holy': 1269, 'horns': 1270, 'horse': 1271, 'hospital': 1272, 'hung': 1273, 'ideals': 1274, 'instead': 1275, 'intellect': 1276, 'intellectual': 1277, 'judgment': 1278, 'kid': 1279, 'lacking': 1280, 'lady': 1281, 'lake': 1282, 'laughed': 1283, 'lead': 1284, 'leading': 1285, 'led': 1286, 'looks': 1287, 'lord': 1288, 'luck': 1289, 'mask': 1290, 'master': 1291, 'meant': 1292, 'methods': 1293, 'moon': 1294, 'mountain': 1295, 'moved': 1296, 'near': 1297, 'necessity': 1298, 'neighbour': 1299, 'neither': 1300, 'nurse': 1301, 'offer': 1302, 'office': 1303, 'officers': 1304, 'opposed': 1305, 'pass': 1306, 'perfectly': 1307, 'personal': 1308, 'phenomenon': 1309, 'pictures': 1310, 'plain': 1311, 'pleasant': 1312, 'poet': 1313, 'poetry': 1314, 'possibly': 1315, 'presented': 1316, 'prison': 1317, 'pull': 1318, 'pure': 1319, 'questions': 1320, 'rank': 1321, 'readily': 1322, 'regarded': 1323, 'religious': 1324, 'require': 1325, 'rid': 1326, 'ride': 1327, 'rule': 1328, 'saint': 1329, 'sentiments': 1330, 'sheet': 1331, 'shell': 1332, 'sign': 1333, 'silence': 1334, 'sit': 1335, 'snow': 1336, 'social': 1337, 'song': 1338, 'spite': 1339, 'spoke': 1340, 'station': 1341, 'stream': 1342, 'suggested': 1343, 'sunlight': 1344, 'system': 1345, 'talked': 1346, 'thick': 1347, 'thinkers': 1348, 'thoughts': 1349, 'thousand': 1350, 'thyself': 1351, 'tonight': 1352, 'triumph': 1353, 'twelve': 1354, 'useful': 1355, 'victory': 1356, 'vision': 1357, 'wake': 1358, 'wall': 1359, 'ways': 1360, 'weary': 1361, 'winter': 1362, 'wood': 1363, 'wooden': 1364, 'wore': 1365, 'works': 1366, 'write': 1367, 'year': 1368, 'yours': 1369, 'Al': 1370, 'Arbuthnot': 1371, 'Austrian': 1372, 'Better': 1373, 'Ca': 1374, 'Christ': 1375, 'DARLINGTON': 1376, 'Erlynne': 1377, 'Everything': 1378, 'Give': 1379, 'Greeks': 1380, 'Has': 1381, 'Hogan': 1382, 'Homer': 1383, 'James': 1384, 'Jesus': 1385, 'MR': 1386, 'Milan': 1387, 'Must': 1388, 'NOT': 1389, 'Nature': 1390, 'Nay': 1391, 'OF': 1392, 'Old': 1393, 'Once': 1394, 'Outside': 1395, 'Rising': 1396, 'Rome': 1397, 'Say': 1398, 'Socratic': 1399, 'Some': 1400, 'Something': 1401, 'Spanish': 1402, 'St.': 1403, 'Street': 1404, 'Sure': 1405, 'Two': 1406, 'Up': 1407, 'Venice': 1408, 'William': 1409, 'Women': 1410, 'action': 1411, 'ad': 1412, 'added': 1413, 'advanced': 1414, 'advantage': 1415, 'afterward': 1416, 'ago': 1417, 'ah': 1418, 'anyway': 1419, 'arise': 1420, 'attempt': 1421, 'attention': 1422, 'attitude': 1423, 'barrera': 1424, 'basis': 1425, 'beast': 1426, 'believes': 1427, 'bodies': 1428, 'bored': 1429, 'bought': 1430, 'brandy': 1431, 'breakfast': 1432, 'breaks': 1433, 'breath': 1434, 'burning': 1435, 'cases': 1436, 'centre': 1437, 'charming': 1438, 'cheerful': 1439, 'chorus': 1440, 'church': 1441, 'clever': 1442, 'cloth': 1443, 'club': 1444, 'coat': 1445, 'colour': 1446, 'compare': 1447, 'concerning': 1448, 'content': 1449, 'continued': 1450, 'create': 1451, 'creating': 1452, 'creatures': 1453, 'cross': 1454, 'crowded': 1455, 'crown': 1456, 'current': 1457, 'daughter': 1458, 'dawn': 1459, 'deception': 1460, 'deepest': 1461, 'degree': 1462, 'delightful': 1463, 'desires': 1464, 'difficult': 1465, 'dirty': 1466, 'dog': 1467, 'drama': 1468, 'dreams': 1469, 'dress': 1470, 'drew': 1471, 'drinking': 1472, 'drop': 1473, 'drove': 1474, 'drunken': 1475, 'ear': 1476, 'easily': 1477, 'education': 1478, 'eleven': 1479, 'emotions': 1480, 'ends': 1481, 'entered': 1482, 'entire': 1483, 'errors': 1484, 'essentially': 1485, 'everybody': 1486, 'everywhere': 1487, 'exact': 1488, 'excellent': 1489, 'exhausted': 1490, 'explain': 1491, 'express': 1492, 'external': 1493, 'fain': 1494, 'falsehood': 1495, 'fancy': 1496, 'fatal': 1497, 'fifty': 1498, 'fighter': 1499, 'flew': 1500, 'follow': 1501, 'fool': 1502, 'fortune': 1503, 'friendship': 1504, 'fruit': 1505, 'fundamental': 1506, 'ghost': 1507, 'gives': 1508, 'grand': 1509, 'grass': 1510, 'greatness': 1511, 'grey': 1512, 'habit': 1513, 'happens': 1514, 'hasty': 1515, 'hence': 1516, 'hero': 1517, 'hills': 1518, 'hurt': 1519, 'imagination': 1520, 'imagine': 1521, 'importance': 1522, 'indignation': 1523, 'individuals': 1524, 'information': 1525, 'jealous': 1526, 'kindly': 1527, 'knees': 1528, 'knoweth': 1529, 'la': 1530, 'lays': 1531, 'leaves': 1532, 'leaving': 1533, 'level': 1534, 'library': 1535, 'lifted': 1536, 'lonely': 1537, 'loose': 1538, 'lower': 1539, 'mad': 1540, 'maybe': 1541, 'metaphysical': 1542, 'method': 1543, 'middle': 1544, 'midnight': 1545, 'minutes': 1546, 'mistake': 1547, 'monstrous': 1548, 'morals': 1549, 'morrow': 1550, 'mothers': 1551, 'moves': 1552, 'myth': 1553, 'naturally': 1554, 'necessarily': 1555, 'nevertheless': 1556, 'news': 1557, 'none': 1558, 'note': 1559, 'noticed': 1560, 'oar': 1561, 'oars': 1562, 'object': 1563, 'obvious': 1564, 'offensive': 1565, 'oneself': 1566, 'ordered': 1567, 'original': 1568, 'owing': 1569, 'painted': 1570, 'painting': 1571, 'passing': 1572, 'pause': 1573, 'peculiar': 1574, 'perfection': 1575, 'physiology': 1576, 'plainly': 1577, 'playing': 1578, 'plenty': 1579, 'pointed': 1580, 'police': 1581, 'porter': 1582, 'practical': 1583, 'practice': 1584, 'prejudices': 1585, 'price': 1586, 'process': 1587, 'quickly': 1588, 'quietly': 1589, 'refuse': 1590, 'relation': 1591, 'responsible': 1592, 'rests': 1593, 'result': 1594, 'reverse': 1595, 'rise': 1596, 'ruin': 1597, 'ruined': 1598, 'runs': 1599, 'sang': 1600, 'save': 1601, 'several': 1602, 'shade': 1603, 'sheets': 1604, 'sickness': 1605, 'silent': 1606, 'similarly': 1607, 'sin': 1608, 'sing': 1609, 'single': 1610, 'size': 1611, 'sky': 1612, 'sleeping': 1613, 'someone': 1614, 'souls': 1615, 'staff': 1616, 'stayed': 1617, 'stern': 1618, 'stretcher': 1619, 'stupid': 1620, 'succeeded': 1621, 'suffer': 1622, 'sufficient': 1623, 'sufficiently': 1624, 'supreme': 1625, 'surface': 1626, 'surprised': 1627, 'swung': 1628, 'tables': 1629, 'talks': 1630, 'tall': 1631, 'taught': 1632, 'teaching': 1633, 'teeth': 1634, 'theory': 1635, 'top': 1636, 'treasure': 1637, 'treat': 1638, 'unity': 1639, 'unknown': 1640, 'vain': 1641, 'valley': 1642, 'values': 1643, 'virtuous': 1644, 'waited': 1645, 'walking': 1646, 'watching': 1647, 'wearing': 1648, 'weight': 1649, 'wholly': 1650, 'wished': 1651, 'woke': 1652, 'wondered': 1653, 'woods': 1654, 'wounds': 1655, 'writing': 1656, 'ARBUTHNOT': 1657, 'AUGUSTUS': 1658, 'Alas': 1659, 'Americans': 1660, 'Any': 1661, 'Are': 1662, 'Barkley': 1663, 'Because': 1664, 'Bonello': 1665, 'Bring': 1666, 'By': 1667, 'Canal': 1668, 'Caroline': 1669, 'Charles': 1670, 'Church': 1671, 'Dear': 1672, 'Dionysus': 1673, 'Dr.': 1674, 'Faust': 1675, 'Finally': 1676, 'Georgette': 1677, 'Goethe': 1678, 'HUNSTANTON': 1679, 'Hello': 1680, 'Hunstanton': 1681, 'ILLINGWORTH': 1682, 'IS': 1683, 'ITS': 1684, 'Indian': 1685, 'Italy': 1686, 'Jackson': 1687, 'Johnson': 1688, 'Justice': 1689, 'Listen': 1690, 'ME': 1691, 'Man': 1692, 'May': 1693, 'Montoya': 1694, 'North': 1695, 'Otis': 1696, 'Picture': 1697, 'Prince': 1698, 'Prism': 1699, 'R.': 1700, 'Richard': 1701, 'Spain': 1702, 'Suddenly': 1703, 'Superman': 1704, 'THIS': 1705, 'TO': 1706, 'Those': 1707, 'Three': 1708, 'Under': 1709, 'Wherever': 1710, 'Windermere': 1711, 'Woman': 1712, 'accord': 1713, 'accounts': 1714, 'aesthetic': 1715, 'aesthetics': 1716, 'agreed': 1717, 'alive': 1718, 'allowed': 1719, 'amount': 1720, 'anyone': 1721, 'anywhere': 1722, 'appear': 1723, 'appearances': 1724, 'army': 1725, 'arrived': 1726, 'artistic': 1727, 'arts': 1728, 'asleep': 1729, 'attacks': 1730, 'author': 1731, 'awake': 1732, 'backwards': 1733, 'barman': 1734, 'battalion': 1735, 'belong': 1736, 'beneath': 1737, 'birds': 1738, 'blessed': 1739, 'bloody': 1740, 'born': 1741, 'boys': 1742, 'breeze': 1743, 'broke': 1744, 'brothers': 1745, 'buried': 1746, 'burned': 1747, 'business': 1748, 'cared': 1749, 'carefully': 1750, 'carriage': 1751, 'cars': 1752, 'catch': 1753, 'cathedral': 1754, 'caused': 1755, 'cautious': 1756, 'celebrated': 1757, 'chairs': 1758, 'changes': 1759, 'choked': 1760, 'choking': 1761, 'civilisation': 1762, 'civilization': 1763, 'classes': 1764, 'clock': 1765, 'clouds': 1766, 'cognac': 1767, 'coldness': 1768, 'commanding': 1769, 'commenced': 1770, 'commit': 1771, 'comparison': 1772, 'completely': 1773, 'conception': 1774, 'condition': 1775, 'conduct': 1776, 'confess': 1777, 'confidence': 1778, 'consciousness': 1779, 'consideration': 1780, 'considered': 1781, 'constitute': 1782, 'contradiction': 1783, 'corridor': 1784, 'corruption': 1785, 'creation': 1786, 'creative': 1787, 'damn': 1788, 'damned': 1789, 'dance': 1790, 'daring': 1791, 'declared': 1792, 'deeds': 1793, 'deeper': 1794, 'demand': 1795, 'deny': 1796, 'des': 1797, 'destiny': 1798, 'device': 1799, 'devoted': 1800, 'difference': 1801, 'difficulty': 1802, 'dim': 1803, 'direction': 1804, 'discourse': 1805, 'discovery': 1806, 'doeth': 1807, 'dollars': 1808, 'drawingroom': 1809, 'dressing': 1810, 'dropped': 1811, 'due': 1812, 'during': 1813, 'easy': 1814, 'echo': 1815, 'effort': 1816, 'elements': 1817, 'emotional': 1818, 'engine': 1819, 'enjoyed': 1820, 'enormous': 1821, 'equal': 1822, 'essay': 1823, 'et': 1824, 'everyone': 1825, 'evolution': 1826, 'exception': 1827, 'exceptional': 1828, 'expressions': 1829, 'fan': 1830, 'fascinated': 1831, 'fat': 1832, 'favour': 1833, 'fields': 1834, 'final': 1835, 'finished': 1836, 'firm': 1837, 'fled': 1838, 'followed': 1839, 'foolish': 1840, 'foot': 1841, 'forehead': 1842, 'forest': 1843, 'forgive': 1844, 'forgot': 1845, 'forgotten': 1846, 'former': 1847, 'formula': 1848, 'fortunate': 1849, 'fought': 1850, 'foundation': 1851, 'fourth': 1852, 'fully': 1853, 'games': 1854, 'gazed': 1855, 'gentle': 1856, 'glance': 1857, 'glimpse': 1858, 'gloomy': 1859, 'glory': 1860, 'gloves': 1861, 'glow': 1862, 'goal': 1863, 'grasp': 1864, 'grave': 1865, 'grief': 1866, 'guests': 1867, 'gun': 1868, 'guns': 1869, 'handle': 1870, 'harm': 1871, 'healthy': 1872, 'hearing': 1873, 'heartless': 1874, 'heaven': 1875, 'henceforth': 1876, 'herd': 1877, 'hide': 1878, 'honest': 1879, 'honey': 1880, 'hoped': 1881, 'hopes': 1882, 'hurry': 1883, 'icy': 1884, 'illusion': 1885, 'imperative': 1886, 'induced': 1887, 'insist': 1888, 'instrument': 1889, 'introduction': 1890, 'involved': 1891, 'join': 1892, 'joke': 1893, 'journey': 1894, 'judge': 1895, 'key': 1896, 'kings': 1897, 'kissed': 1898, 'ladder': 1899, 'larger': 1900, 'leaning': 1901, 'leather': 1902, 'lift': 1903, 'listening': 1904, 'lit': 1905, 'literature': 1906, 'liveth': 1907, 'longest': 1908, 'longing': 1909, 'lordship': 1910, 'loss': 1911, 'lover': 1912, 'loving': 1913, 'lowest': 1914, 'lust': 1915, 'market': 1916, 'marketplace': 1917, 'mass': 1918, 'matters': 1919, 'melancholy': 1920, 'miserable': 1921, 'misunderstand': 1922, 'misunderstanding': 1923, 'muleta': 1924, 'mystery': 1925, 'natural': 1926, 'needed': 1927, 'negative': 1928, 'nerves': 1929, 'nervous': 1930, 'non': 1931, 'notice': 1932, 'notion': 1933, 'number': 1934, 'obscene': 1935, 'observation': 1936, 'offered': 1937, 'older': 1938, 'opinions': 1939, 'opponents': 1940, 'page': 1941, 'painter': 1942, 'particularly': 1943, 'partly': 1944, 'parts': 1945, 'phenomena': 1946, 'philosophical': 1947, 'phrase': 1948, 'pieces': 1949, 'places': 1950, 'played': 1951, 'pleased': 1952, 'popular': 1953, 'poured': 1954, 'prefer': 1955, 'prepared': 1956, 'presence': 1957, 'principle': 1958, 'privilege': 1959, 'prizes': 1960, 'problems': 1961, 'produces': 1962, 'promise': 1963, 'promised': 1964, 'proof': 1965, 'properly': 1966, 'propose': 1967, 'proposition': 1968, 'psychologist': 1969, 'purely': 1970, 'purple': 1971, 'pushed': 1972, 'qualities': 1973, 'quality': 1974, 'races': 1975, 'raised': 1976, 'ranch': 1977, 'rang': 1978, 'rare': 1979, 'rational': 1980, 'reach': 1981, 'reaching': 1982, 'reflection': 1983, 'regards': 1984, 'remained': 1985, 'repeated': 1986, 'requires': 1987, 'rested': 1988, 'results': 1989, 'returned': 1990, 'ripe': 1991, 'romantic': 1992, 'roofs': 1993, 'rough': 1994, 'rude': 1995, 'ruling': 1996, 'sagacity': 1997, 'saints': 1998, 'satisfaction': 1999, 'saucer': 2000, 'screen': 2001, 'secrets': 2002, 'seeing': 2003, 'seeking': 2004, 'senses': 2005, 'seriousness': 2006, 'serpent': 2007, 'seven': 2008, 'shake': 2009, 'shalt': 2010, 'shine': 2011, 'shoulder': 2012, 'singing': 2013, 'sleepy': 2014, 'slept': 2015, 'slipped': 2016, 'smelled': 2017, 'sofa': 2018, 'soft': 2019, 'sold': 2020, 'soldiers': 2021, 'solitude': 2022, 'somewhat': 2023, 'sounds': 2024, 'spent': 2025, 'spirituality': 2026, 'splendid': 2027, 'spoiled': 2028, 'spoken': 2029, 'spots': 2030, 'standpoint': 2031, 'stars': 2032, 'statue': 2033, 'staying': 2034, 'stepped': 2035, 'steps': 2036, 'stiff': 2037, 'stomach': 2038, 'story': 2039, 'stranger': 2040, 'strongest': 2041, 'style': 2042, 'sublime': 2043, 'supper': 2044, 'suspect': 2045, 'tail': 2046, 'task': 2047, 'tedious': 2048, 'terrace': 2049, 'thoroughly': 2050, 'throat': 2051, 'tone': 2052, 'track': 2053, 'treated': 2054, 'truck': 2055, 'trucks': 2056, 'truthfulness': 2057, 'tunc': 2058, 'turns': 2059, 'turtles': 2060, 'ugliest': 2061, 'ultimate': 2062, 'uncertain': 2063, 'unfortunate': 2064, 'unjust': 2065, 'upstairs': 2066, 'usual': 2067, 'vast': 2068, 'victorious': 2069, 'vulgar': 2070, 'warm': 2071, 'wear': 2072, 'wheels': 2073, 'wide': 2074, 'wilderness': 2075, 'winning': 2076, 'woe': 2077, 'womans': 2078, 'worlds': 2079, 'worship': 2080, 'worthy': 2081, 'writer': 2082, 'yellow': 2083, 'yesterday': 2084, '1': 2085, '25': 2086, 'ACT': 2087, 'AND': 2088, 'Above': 2089, 'Against': 2090, 'Alan': 2091, 'Also': 2092, 'Always': 2093, 'Am': 2094, 'Another': 2095, 'Army': 2096, 'Arthur': 2097, 'Ashley': 2098, 'Ask': 2099, 'August': 2100, 'BE': 2101, 'Before': 2102, 'Book': 2103, 'Boulevard': 2104, 'CAN': 2105, 'CAVERSHAM': 2106, 'CECIL': 2107, 'CHILDREN': 2108, 'Cabinet': 2109, 'Carl': 2110, 'Caversham': 2111, 'Cayetano': 2112, 'Cecil': 2113, 'Certainly': 2114, 'Charming': 2115, 'Chicago': 2116, 'City': 2117, 'Courage': 2118, 'Court': 2119, 'Culture': 2120, 'Darling': 2121, 'Edna': 2122, 'Englishman': 2123, 'Erskine': 2124, 'Everybody': 2125, 'Everywhere': 2126, 'Evil': 2127, 'Five': 2128, 'Frances': 2129, 'Frederick': 2130, 'GRAHAM': 2131, 'Giant': 2132, 'Going': 2133, 'Goring': 2134, 'Gritti': 2135, 'Gwendolen': 2136, 'HESTER': 2137, 'Happy': 2138, 'Harris': 2139, 'Harvey': 2140, 'Hath': 2141, 'Hellenic': 2142, 'Herr': 2143, 'However': 2144, 'Hubbard': 2145, 'I.': 2146, 'IN': 2147, 'Indians': 2148, 'Italians': 2149, 'Jacks': 2150, 'Jew': 2151, 'Jewish': 2152, 'Jim': 2153, 'Joe': 2154, 'Kantian': 2155, 'King': 2156, 'La': 2157, 'Later': 2158, 'Lewis': 2159, 'Linnet': 2160, 'Little': 2161, 'Live': 2162, 'Look': 2163, 'Love': 2164, 'Many': 2165, 'Mary': 2166, 'Max': 2167, 'Meyers': 2168, 'Minister': 2169, 'Morality': 2170, 'Most': 2171, 'Music': 2172, 'Neither': 2173, 'Nobody': 2174, 'Park': 2175, 'Peduzzi': 2176, 'People': 2177, 'Philistine': 2178, 'Piani': 2179, 'Portrait': 2180, 'Power': 2181, 'Priest': 2182, 'Probably': 2183, 'Rachel': 2184, 'Rather': 2185, 'Really': 2186, 'Schumann': 2187, 'Scotland': 2188, 'Should': 2189, 'Signor': 2190, 'Six': 2191, 'Society': 2192, 'Sometimes': 2193, 'South': 2194, 'Still': 2195, 'Stop': 2196, 'Take': 2197, 'Testament': 2198, 'Therefore': 2199, 'Today': 2200, 'Tom': 2201, 'Tommy': 2202, 'Truth': 2203, 'Uncle': 2204, 'Very': 2205, 'Virtue': 2206, 'WHAT': 2207, 'Want': 2208, 'Whatever': 2209, 'Which': 2210, 'Wilcox': 2211, 'Without': 2212, 'YOU': 2213, 'York': 2214, 'abandoned': 2215, 'absorbed': 2216, 'absurdity': 2217, 'abyss': 2218, 'acknowledge': 2219, 'acquired': 2220, 'acting': 2221, 'actions': 2222, 'admiration': 2223, 'admit': 2224, 'advance': 2225, 'affectionate': 2226, 'afterwards': 2227, 'aloft': 2228, 'altruistic': 2229, 'analogous': 2230, 'angry': 2231, 'animal': 2232, 'answers': 2233, 'apparent': 2234, 'apparently': 2235, 'appearing': 2236, 'appreciate': 2237, 'arrested': 2238, 'artillery': 2239, 'ascended': 2240, 'ascending': 2241, 'aspect': 2242, 'astonishment': 2243, 'aut': 2244, 'autobiography': 2245, 'awoke': 2246, 'bags': 2247, 'bank': 2248, 'bare': 2249, 'barrel': 2250, 'bath': 2251, 'bathroom': 2252, 'beach': 2253, 'beard': 2254, 'beg': 2255, 'begins': 2256, 'behaves': 2257, 'bell': 2258, 'belly': 2259, 'bending': 2260, 'bent': 2261, 'blade': 2262, 'bleeding': 2263, 'blindness': 2264, 'blow': 2265, 'blowing': 2266, 'blows': 2267, 'bold': 2268, 'bombardment': 2269, 'boneheap': 2270, 'bore': 2271, 'bores': 2272, 'bottles': 2273, 'boxes': 2274, 'branches': 2275, 'brass': 2276, 'brave': 2277, 'breast': 2278, 'brief': 2279, 'bucket': 2280, 'build': 2281, 'buy': 2282, 'calculated': 2283, 'calls': 2284, 'calm': 2285, 'cap': 2286, 'card': 2287, 'cash': 2288, 'castes': 2289, 'cautiously': 2290, 'censure': 2291, 'centuries': 2292, 'characteristic': 2293, 'charge': 2294, 'charity': 2295, 'charm': 2296, 'chastity': 2297, 'cheek': 2298, 'cheeks': 2299, 'cheese': 2300, 'chief': 2301, 'choice': 2302, 'chose': 2303, 'christened': 2304, 'cigarette': 2305, 'clearly': 2306, 'climate': 2307, 'climb': 2308, 'climbed': 2309, 'closed': 2310, 'closer': 2311, 'clumsy': 2312, 'colours': 2313, 'cometh': 2314, 'commands': 2315, 'complex': 2316, 'composition': 2317, 'concealed': 2318, 'concept': 2319, 'confession': 2320, 'conflict': 2321, 'confused': 2322, 'conjure': 2323, 'connected': 2324, 'consensus': 2325, 'consequence': 2326, 'consider': 2327, 'considerably': 2328, 'considering': 2329, 'consists': 2330, 'consolation': 2331, 'constantly': 2332, 'constitutes': 2333, 'continually': 2334, 'control': 2335, 'convince': 2336, 'copy': 2337, 'countenance': 2338, 'counter': 2339, 'cover': 2340, 'crawled': 2341, 'crazy': 2342, 'creator': 2343, 'crime': 2344, 'crushed': 2345, 'damp': 2346, 'dancers': 2347, 'dangers': 2348, 'decadent': 2349, 'decided': 2350, 'decoys': 2351, 'degenerate': 2352, 'degeneration': 2353, 'degrees': 2354, 'delicacy': 2355, 'delicate': 2356, 'demmed': 2357, 'democratic': 2358, 'denial': 2359, 'depends': 2360, 'despised': 2361, 'destroyed': 2362, 'dew': 2363, 'diamonds': 2364, 'differently': 2365, 'dignity': 2366, 'dine': 2367, 'dipping': 2368, 'directed': 2369, 'disappointed': 2370, 'discipline': 2371, 'discontent': 2372, 'disgrace': 2373, 'disgust': 2374, 'dislike': 2375, 'distinguished': 2376, 'distress': 2377, 'distressed': 2378, 'disturbing': 2379, 'doctrine': 2380, 'dogs': 2381, 'domain': 2382, 'domestic': 2383, 'dominated': 2384, 'doors': 2385, 'doorway': 2386, 'drinks': 2387, 'ducks': 2388, 'dug': 2389, 'dull': 2390, 'dust': 2391, 'dusty': 2392, 'duties': 2393, 'eagle': 2394, 'ecstasy': 2395, 'electric': 2396, 'elevator': 2397, 'emphasis': 2398, 'enjoy': 2399, 'entrance': 2400, 'equally': 2401, 'est': 2402, 'established': 2403, 'estimates': 2404, 'eternity': 2405, 'exaggerated': 2406, 'exalted': 2407, 'exchange': 2408, 'exist': 2409, 'expected': 2410, 'experienced': 2411, 'experiment': 2412, 'explanation': 2413, 'expressed': 2414, 'facts': 2415, 'faculty': 2416, 'failure': 2417, 'falling': 2418, 'famous': 2419, 'farm': 2420, 'fault': 2421, 'faults': 2422, 'favourable': 2423, 'feared': 2424, 'fiction': 2425, 'field': 2426, 'fiesta': 2427, 'fights': 2428, 'finding': 2429, 'finest': 2430, 'finger': 2431, 'firing': 2432, 'fishing': 2433, 'fixed': 2434, 'flask': 2435, 'folded': 2436, 'follies': 2437, 'folly': 2438, 'food': 2439, 'football': 2440, 'forbidden': 2441, 'forces': 2442, 'forgiven': 2443, 'formally': 2444, 'formerly': 2445, 'forsooth': 2446, 'forth': 2447, 'francs': 2448, 'friendly': 2449, 'fun': 2450, 'funeral': 2451, 'gained': 2452, 'gas': 2453, 'gazing': 2454, 'genuine': 2455, 'grammar': 2456, 'granted': 2457, 'grasped': 2458, 'gravity': 2459, 'grotesque': 2460, 'group': 2461, 'hall': 2462, 'hang': 2463, 'hanging': 2464, 'harder': 2465, 'harmony': 2466, 'hastily': 2467, 'healing': 2468, 'health': 2469, 'healthiness': 2470, 'heel': 2471, 'hells': 2472, 'helped': 2473, 'herald': 2474, 'heretofore': 2475, 'hesitate': 2476, 'hideous': 2477, 'holds': 2478, 'hollow': 2479, 'hook': 2480, 'horn': 2481, 'horribly': 2482, 'hostile': 2483, 'hunting': 2484, 'hypotheses': 2485, 'i.': 2486, 'immense': 2487, 'immensely': 2488, 'immoral': 2489, 'imperfectly': 2490, 'impulse': 2491, 'impulses': 2492, 'inclination': 2493, 'inclined': 2494, 'incomprehensible': 2495, 'infinite': 2496, 'injury': 2497, 'innocence': 2498, 'insists': 2499, 'inspire': 2500, 'inspired': 2501, 'instruments': 2502, 'intelligible': 2503, 'intention': 2504, 'intercourse': 2505, 'interest': 2506, 'interpretation': 2507, 'introduce': 2508, 'invariably': 2509, 'jam': 2510, 'journalist': 2511, 'judgments': 2512, 'keeping': 2513, 'kike': 2514, 'killing': 2515, 'kine': 2516, 'king': 2517, 'kitchen': 2518, 'labour': 2519, 'lad': 2520, 'land': 2521, 'lately': 2522, 'leads': 2523, 'learnt': 2524, 'lefthand': 2525, 'leg': 2526, 'lick': 2527, 'lieth': 2528, 'lieutenant': 2529, 'lifetask': 2530, 'lightning': 2531, 'likes': 2532, 'lofty': 2533, 'logic': 2534, 'lonesomeness': 2535, 'longed': 2536, 'loveth': 2537, 'lucky': 2538, 'lure': 2539, 'lyre': 2540, 'madness': 2541, 'magic': 2542, 'maintain': 2543, 'manifests': 2544, 'manners': 2545, 'mar': 2546, 'marred': 2547, 'marriage': 2548, 'marvellous': 2549, 'mastery': 2550, 'maxim': 2551, 'mechanical': 2552, 'memory': 2553, 'mess': 2554, 'midst': 2555, 'mild': 2556, 'mile': 2557, 'mill': 2558, 'minute': 2559, 'mirror': 2560, 'missed': 2561, 'missing': 2562, 'mistaken': 2563, 'modesty': 2564, 'month': 2565, 'mules': 2566, 'multitude': 2567, 'musical': 2568, 'nation': 2569, 'nations': 2570, 'natures': 2571, 'nearest': 2572, 'newspapers': 2573, 'nine': 2574, 'nineteenth': 2575, 'nobody': 2576, 'nodded': 2577, 'north': 2578, 'objective': 2579, 'obtained': 2580, 'official': 2581, 'oneness': 2582, 'opposition': 2583, 'oppressed': 2584, 'optimism': 2585, 'orders': 2586, 'originally': 2587, 'otherwise': 2588, 'outbursts': 2589, 'outer': 2590, 'owe': 2591, 'painters': 2592, 'pair': 2593, 'parable': 2594, 'parting': 2595, 'passionate': 2596, 'path': 2597, 'paused': 2598, 'pays': 2599, 'pen': 2600, 'per': 2601, 'persistent': 2602, 'petty': 2603, 'phantom': 2604, 'physician': 2605, 'physiological': 2606, 'piled': 2607, 'pitcher': 2608, 'pitiable': 2609, 'pitiful': 2610, 'poetical': 2611, 'poets': 2612, 'points': 2613, 'politics': 2614, 'populace': 2615, 'possessed': 2616, 'powers': 2617, 'praised': 2618, 'prayer': 2619, 'preparations': 2620, 'prepare': 2621, 'preservation': 2622, 'previous': 2623, 'prey': 2624, 'priests': 2625, 'principles': 2626, 'profoundly': 2627, 'proudly': 2628, 'prove': 2629, 'provided': 2630, 'punished': 2631, 'punishment': 2632, 'purity': 2633, 'puts': 2634, 'quarters': 2635, 'race': 2636, 'racing': 2637, 'radical': 2638, 'rapidly': 2639, 'rarely': 2640, 'ray': 2641, 'realize': 2642, 'realm': 2643, 'reasons': 2644, 'received': 2645, 'recognised': 2646, 'recognized': 2647, 'reconciliation': 2648, 'referred': 2649, 'refined': 2650, 'refrain': 2651, 'regiment': 2652, 'remind': 2653, 'remorse': 2654, 'remotest': 2655, 'render': 2656, 'replied': 2657, 'respectable': 2658, 'responsibility': 2659, 'restaurant': 2660, 'retreat': 2661, 'reveal': 2662, 'reveals': 2663, 'revelation': 2664, 'ridiculous': 2665, 'ripped': 2666, 'rising': 2667, 'rolls': 2668, 'rooms': 2669, 'roses': 2670, 'rot': 2671, 'ruby': 2672, 'rush': 2673, 'sand': 2674, 'satisfy': 2675, 'scarcely': 2676, 'scarlet': 2677, 'scheme': 2678, 'scholar': 2679, 'seated': 2680, 'secretary': 2681, 'seek': 2682, 'seldom': 2683, 'selfish': 2684, 'sensitive': 2685, 'sentimental': 2686, 'separate': 2687, 'separated': 2688, 'serenity': 2689, 'servants': 2690, 'setting': 2691, 'severe': 2692, 'sexual': 2693, 'shape': 2694, 'shark': 2695, 'shed': 2696, 'shells': 2697, 'shield': 2698, 'ship': 2699, 'shooter': 2700, 'shop': 2701, 'shout': 2702, 'showed': 2703, 'showing': 2704, 'shown': 2705, 'shudder': 2706, 'simplicity': 2707, 'sister': 2708, 'skepticism': 2709, 'sleeve': 2710, 'slip': 2711, 'slothfulness': 2712, 'smoke': 2713, 'somebody': 2714, 'sooner': 2715, 'source': 2716, 'speaketh': 2717, 'species': 2718, 'spectator': 2719, 'spectators': 2720, 'speech': 2721, 'spell': 2722, 'spiritual': 2723, 'splashed': 2724, 'splendour': 2725, 'sport': 2726, 'stands': 2727, 'star': 2728, 'starts': 2729, 'steady': 2730, 'steak': 2731, 'steal': 2732, 'stock': 2733, 'stole': 2734, 'stones': 2735, 'stool': 2736, 'stored': 2737, 'strangeness': 2738, 'streets': 2739, 'stripes': 2740, 'strongly': 2741, 'struck': 2742, 'stupidity': 2743, 'subtle': 2744, 'successful': 2745, 'suffered': 2746, 'sufferers': 2747, 'sufferest': 2748, 'suit': 2749, 'summer': 2750, 'superfluous': 2751, 'support': 2752, 'supposed': 2753, 'surely': 2754, 'surprise': 2755, 'suspicion': 2756, 'suspicious': 2757, 'swim': 2758, 'sword': 2759, 'sympathetic': 2760, 'systems': 2761, 'tender': 2762, 'terms': 2763, 'terribly': 2764, 'thanks': 2765, 'theatre': 2766, 'thereof': 2767, 'thirty': 2768, 'tied': 2769, 'till': 2770, 'tiller': 2771, 'timber': 2772, 'tiny': 2773, 'tomorrow': 2774, 'tones': 2775, 'tongue': 2776, 'torch': 2777, 'tossed': 2778, 'tough': 2779, 'transcendental': 2780, 'transfiguring': 2781, 'translated': 2782, 'treatise': 2783, 'tree': 2784, 'tremble': 2785, 'trembling': 2786, 'troops': 2787, 'trout': 2788, 'truths': 2789, 'twenty': 2790, 'ugliness': 2791, 'umbrella': 2792, 'unawares': 2793, 'unconscious': 2794, 'underneath': 2795, 'understanding': 2796, 'understands': 2797, 'universal': 2798, 'unkind': 2799, 'using': 2800, 'utility': 2801, 'vague': 2802, 'valuation': 2803, 'vanity': 2804, 'various': 2805, 'veil': 2806, 'verses': 2807, 'violent': 2808, 'visible': 2809, 'visionary': 2810, 'voices': 2811, 'wagon': 2812, 'waiters': 2813, 'waitress': 2814, 'wanteth': 2815, 'wanton': 2816, 'wash': 2817, 'waters': 2818, 'waves': 2819, 'weakness': 2820, 'weep': 2821, 'welcome': 2822, 'wherever': 2823, 'whisper': 2824, 'willingly': 2825, 'wilt': 2826, 'wings': 2827, 'winner': 2828, 'wishes': 2829, 'withered': 2830, 'workers': 2831, 'worldly': 2832, 'wounded': 2833, 'writings': 2834, 'younger': 2835})\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train_data,min_freq=3,vectors = \"glove.6B.300d\")  \n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "#No. of unique tokens in text\n",
    "print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n",
    "\n",
    "#No. of unique tokens in label\n",
    "print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\n",
    "\n",
    "#Commonly used words\n",
    "print(TEXT.vocab.freqs.most_common(10))  \n",
    "\n",
    "#Word dictionary\n",
    "print(TEXT.vocab.stoi)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
    "\n",
    "#set batch size\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_key = lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, weights):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_length = embedding_length\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_length)# Initializing the look-up table.\n",
    "        self.word_embeddings.weight = nn.Parameter(weights, requires_grad=False) # Assigning the look-up table to the pre-trained GloVe word embedding.\n",
    "        self.lstm = nn.LSTM(embedding_length, hidden_size)\n",
    "        self.label = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, input_sentence, batch_size=None):\n",
    "        \n",
    "        ''' Here we will map all the indexes present in the input sequence to the corresponding word vector using our pre-trained word_embedddins.'''\n",
    "        input = self.word_embeddings(input_sentence) # embedded input of shape = (batch_size, num_sequences,  embedding_length)\n",
    "        input = input.permute(1, 0, 2) # input.size() = (num_sequences, batch_size, embedding_length)\n",
    "        if batch_size is None:\n",
    "            h_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).cuda()) # Initial hidden state of the LSTM\n",
    "            c_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).cuda()) # Initial cell state of the LSTM\n",
    "        else:\n",
    "            h_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\n",
    "            c_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\n",
    "        output, (final_hidden_state, final_cell_state) = self.lstm(input, (h_0, c_0))\n",
    "        final_output = self.label(final_hidden_state[-1]) # final_hidden_state.size() = (1, batch_size, hidden_size) & final_output.size() = (batch_size, output_size)\n",
    "        \n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradient(model, clip_value):\n",
    "    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
    "    for p in params:\n",
    "        p.grad.data.clamp_(-clip_value, clip_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_iter, epoch):\n",
    "    total_epoch_loss = 0\n",
    "    total_epoch_acc = 0\n",
    "    model.cuda()\n",
    "    optim = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "    steps = 0\n",
    "    model.train()\n",
    "    for idx, batch in enumerate(train_iter):\n",
    "        #print(\"label\",batch.label)\n",
    "        text = batch.text[0]\n",
    "        target = batch.label\n",
    "        target = torch.autograd.Variable(target).long()\n",
    "        if torch.cuda.is_available():\n",
    "            text = text.cuda()\n",
    "            target = target.cuda()\n",
    "        if (text.size()[0] is not 32):# One of the batch returned by BucketIterator has length different than 32.\n",
    "            continue\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        prediction = model(text)\n",
    "        loss = loss_fn(prediction, target)\n",
    "        num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).float().sum()\n",
    "        acc = 100.0 * num_corrects/len(batch)\n",
    "        loss.backward()\n",
    "        clip_gradient(model, 1e-1)\n",
    "        optim.step()\n",
    "        steps += 1\n",
    "        \n",
    "        if steps % 100 == 0:\n",
    "            print (f'Epoch: {epoch+1}, Idx: {idx+1}, Training Loss: {loss.item():.4f}, Training Accuracy: {acc.item(): .2f}%')\n",
    "        \n",
    "        total_epoch_loss += loss.item()\n",
    "        total_epoch_acc += acc.item()\n",
    "        \n",
    "    return total_epoch_loss/len(train_iter), total_epoch_acc/len(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, val_iter):\n",
    "    total_epoch_loss = 0\n",
    "    total_epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(val_iter):\n",
    "            text = batch.text[0]\n",
    "            if (text.size()[0] is not 32):\n",
    "                continue\n",
    "            target = batch.label\n",
    "            target = torch.autograd.Variable(target).long()\n",
    "            if torch.cuda.is_available():\n",
    "                text = text.cuda()\n",
    "                target = target.cuda()\n",
    "            prediction = model(text)\n",
    "            loss = loss_fn(prediction, target)\n",
    "            num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).sum()\n",
    "            acc = 100.0 * num_corrects/len(batch)\n",
    "            total_epoch_loss += loss.item()\n",
    "            total_epoch_acc += acc.item()\n",
    "\n",
    "    return total_epoch_loss/len(val_iter), total_epoch_acc/len(val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 2e-5\n",
    "batch_size = 32\n",
    "output_size = 3\n",
    "hidden_size = 256\n",
    "embedding_length = 300\n",
    "vocab_size = len(TEXT.vocab)\n",
    "word_embeddings = TEXT.vocab.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMClassifier(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
    "loss_fn = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Idx: 100, Training Loss: 0.5556, Training Accuracy:  71.88%\n",
      "Epoch: 01, Train Loss: 0.392, Train Acc: 83.53%, Val. Loss: 0.582974, Val. Acc: 76.61%\n",
      "Epoch: 2, Idx: 100, Training Loss: 0.4323, Training Accuracy:  78.12%\n",
      "Epoch: 02, Train Loss: 0.364, Train Acc: 84.86%, Val. Loss: 0.605530, Val. Acc: 75.26%\n",
      "Epoch: 3, Idx: 100, Training Loss: 0.5818, Training Accuracy:  62.50%\n",
      "Epoch: 03, Train Loss: 0.327, Train Acc: 86.49%, Val. Loss: 0.648895, Val. Acc: 75.82%\n",
      "Epoch: 4, Idx: 100, Training Loss: 0.3782, Training Accuracy:  90.62%\n",
      "Epoch: 04, Train Loss: 0.297, Train Acc: 87.68%, Val. Loss: 0.654340, Val. Acc: 76.08%\n",
      "Epoch: 5, Idx: 100, Training Loss: 0.1703, Training Accuracy:  90.62%\n",
      "Epoch: 05, Train Loss: 0.247, Train Acc: 89.68%, Val. Loss: 0.770465, Val. Acc: 75.78%\n",
      "Epoch: 6, Idx: 100, Training Loss: 0.1246, Training Accuracy:  100.00%\n",
      "Epoch: 06, Train Loss: 0.221, Train Acc: 91.19%, Val. Loss: 0.801234, Val. Acc: 76.26%\n",
      "Epoch: 7, Idx: 100, Training Loss: 0.0571, Training Accuracy:  100.00%\n",
      "Epoch: 07, Train Loss: 0.213, Train Acc: 91.49%, Val. Loss: 0.883355, Val. Acc: 74.64%\n",
      "Epoch: 8, Idx: 100, Training Loss: 0.2197, Training Accuracy:  90.62%\n",
      "Epoch: 08, Train Loss: 0.230, Train Acc: 91.35%, Val. Loss: 0.810826, Val. Acc: 75.95%\n",
      "Epoch: 9, Idx: 100, Training Loss: 0.0808, Training Accuracy:  96.88%\n",
      "Epoch: 09, Train Loss: 0.187, Train Acc: 92.78%, Val. Loss: 0.895725, Val. Acc: 75.02%\n",
      "Epoch: 10, Idx: 100, Training Loss: 0.6540, Training Accuracy:  87.50%\n",
      "Epoch: 10, Train Loss: 0.159, Train Acc: 93.53%, Val. Loss: 0.895315, Val. Acc: 75.07%\n",
      "Epoch: 11, Idx: 100, Training Loss: 0.0493, Training Accuracy:  100.00%\n",
      "Epoch: 11, Train Loss: 0.138, Train Acc: 94.52%, Val. Loss: 0.980533, Val. Acc: 75.26%\n",
      "Epoch: 12, Idx: 100, Training Loss: 0.1364, Training Accuracy:  96.88%\n",
      "Epoch: 12, Train Loss: 0.146, Train Acc: 94.29%, Val. Loss: 1.000520, Val. Acc: 75.11%\n",
      "Epoch: 13, Idx: 100, Training Loss: 0.0616, Training Accuracy:  100.00%\n",
      "Epoch: 13, Train Loss: 0.119, Train Acc: 95.16%, Val. Loss: 1.025082, Val. Acc: 73.99%\n",
      "Epoch: 14, Idx: 100, Training Loss: 0.0144, Training Accuracy:  100.00%\n",
      "Epoch: 14, Train Loss: 0.104, Train Acc: 95.56%, Val. Loss: 1.099939, Val. Acc: 75.51%\n",
      "Epoch: 15, Idx: 100, Training Loss: 0.0291, Training Accuracy:  100.00%\n",
      "Epoch: 15, Train Loss: 0.091, Train Acc: 95.97%, Val. Loss: 1.186504, Val. Acc: 74.17%\n",
      "Epoch: 16, Idx: 100, Training Loss: 0.0959, Training Accuracy:  93.75%\n",
      "Epoch: 16, Train Loss: 0.084, Train Acc: 96.41%, Val. Loss: 1.134365, Val. Acc: 74.80%\n",
      "Epoch: 17, Idx: 100, Training Loss: 0.0217, Training Accuracy:  100.00%\n",
      "Epoch: 17, Train Loss: 0.086, Train Acc: 96.49%, Val. Loss: 1.176043, Val. Acc: 74.32%\n",
      "Epoch: 18, Idx: 100, Training Loss: 0.0103, Training Accuracy:  100.00%\n",
      "Epoch: 18, Train Loss: 0.065, Train Acc: 97.30%, Val. Loss: 1.380375, Val. Acc: 74.32%\n",
      "Epoch: 19, Idx: 100, Training Loss: 0.0339, Training Accuracy:  100.00%\n",
      "Epoch: 19, Train Loss: 0.076, Train Acc: 96.77%, Val. Loss: 1.250819, Val. Acc: 73.98%\n",
      "Epoch: 20, Idx: 100, Training Loss: 0.0051, Training Accuracy:  100.00%\n",
      "Epoch: 20, Train Loss: 0.075, Train Acc: 96.94%, Val. Loss: 1.298229, Val. Acc: 74.10%\n",
      "Epoch: 21, Idx: 100, Training Loss: 0.0867, Training Accuracy:  96.88%\n",
      "Epoch: 21, Train Loss: 0.056, Train Acc: 97.52%, Val. Loss: 1.294704, Val. Acc: 73.99%\n",
      "Epoch: 22, Idx: 100, Training Loss: 0.3776, Training Accuracy:  93.75%\n",
      "Epoch: 22, Train Loss: 0.060, Train Acc: 97.34%, Val. Loss: 1.350922, Val. Acc: 74.58%\n",
      "Epoch: 23, Idx: 100, Training Loss: 0.0078, Training Accuracy:  100.00%\n",
      "Epoch: 23, Train Loss: 0.063, Train Acc: 97.14%, Val. Loss: 1.336673, Val. Acc: 74.30%\n",
      "Epoch: 24, Idx: 100, Training Loss: 0.0072, Training Accuracy:  100.00%\n",
      "Epoch: 24, Train Loss: 0.059, Train Acc: 97.54%, Val. Loss: 1.327302, Val. Acc: 74.41%\n",
      "Epoch: 25, Idx: 100, Training Loss: 0.0267, Training Accuracy:  100.00%\n",
      "Epoch: 25, Train Loss: 0.043, Train Acc: 97.94%, Val. Loss: 1.389192, Val. Acc: 74.18%\n",
      "Epoch: 26, Idx: 100, Training Loss: 0.0427, Training Accuracy:  100.00%\n",
      "Epoch: 26, Train Loss: 0.055, Train Acc: 97.78%, Val. Loss: 1.433274, Val. Acc: 74.48%\n",
      "Epoch: 27, Idx: 100, Training Loss: 0.0040, Training Accuracy:  100.00%\n",
      "Epoch: 27, Train Loss: 0.053, Train Acc: 97.66%, Val. Loss: 1.352111, Val. Acc: 75.04%\n",
      "Epoch: 28, Idx: 100, Training Loss: 0.2837, Training Accuracy:  90.62%\n",
      "Epoch: 28, Train Loss: 0.060, Train Acc: 97.30%, Val. Loss: 1.338677, Val. Acc: 74.50%\n",
      "Epoch: 29, Idx: 100, Training Loss: 0.0094, Training Accuracy:  100.00%\n",
      "Epoch: 29, Train Loss: 0.040, Train Acc: 98.00%, Val. Loss: 1.467293, Val. Acc: 72.27%\n",
      "Epoch: 30, Idx: 100, Training Loss: 0.0160, Training Accuracy:  100.00%\n",
      "Epoch: 30, Train Loss: 0.046, Train Acc: 97.82%, Val. Loss: 1.383057, Val. Acc: 73.93%\n",
      "Epoch: 31, Idx: 100, Training Loss: 0.0491, Training Accuracy:  96.88%\n",
      "Epoch: 31, Train Loss: 0.039, Train Acc: 98.08%, Val. Loss: 1.423613, Val. Acc: 73.82%\n",
      "Epoch: 32, Idx: 100, Training Loss: 0.0211, Training Accuracy:  100.00%\n",
      "Epoch: 32, Train Loss: 0.038, Train Acc: 98.19%, Val. Loss: 1.469159, Val. Acc: 73.73%\n",
      "Epoch: 33, Idx: 100, Training Loss: 0.0050, Training Accuracy:  100.00%\n",
      "Epoch: 33, Train Loss: 0.030, Train Acc: 98.49%, Val. Loss: 1.489322, Val. Acc: 74.04%\n",
      "Epoch: 34, Idx: 100, Training Loss: 0.0007, Training Accuracy:  100.00%\n",
      "Epoch: 34, Train Loss: 0.029, Train Acc: 98.59%, Val. Loss: 1.490321, Val. Acc: 74.17%\n",
      "Epoch: 35, Idx: 100, Training Loss: 0.0019, Training Accuracy:  100.00%\n",
      "Epoch: 35, Train Loss: 0.035, Train Acc: 98.29%, Val. Loss: 1.559836, Val. Acc: 73.71%\n",
      "Epoch: 36, Idx: 100, Training Loss: 0.0141, Training Accuracy:  100.00%\n",
      "Epoch: 36, Train Loss: 0.041, Train Acc: 98.21%, Val. Loss: 1.532894, Val. Acc: 72.40%\n",
      "Epoch: 37, Idx: 100, Training Loss: 0.1478, Training Accuracy:  96.88%\n",
      "Epoch: 37, Train Loss: 0.030, Train Acc: 98.39%, Val. Loss: 1.652102, Val. Acc: 74.49%\n",
      "Epoch: 38, Idx: 100, Training Loss: 0.0097, Training Accuracy:  100.00%\n",
      "Epoch: 38, Train Loss: 0.031, Train Acc: 98.49%, Val. Loss: 1.612025, Val. Acc: 74.20%\n",
      "Epoch: 39, Idx: 100, Training Loss: 0.0055, Training Accuracy:  100.00%\n",
      "Epoch: 39, Train Loss: 0.023, Train Acc: 98.53%, Val. Loss: 1.544479, Val. Acc: 74.72%\n",
      "Epoch: 40, Idx: 100, Training Loss: 0.0076, Training Accuracy:  100.00%\n",
      "Epoch: 40, Train Loss: 0.042, Train Acc: 98.19%, Val. Loss: 1.521540, Val. Acc: 73.95%\n",
      "Epoch: 41, Idx: 100, Training Loss: 0.0006, Training Accuracy:  100.00%\n",
      "Epoch: 41, Train Loss: 0.033, Train Acc: 98.23%, Val. Loss: 1.672711, Val. Acc: 74.30%\n",
      "Epoch: 42, Idx: 100, Training Loss: 0.0139, Training Accuracy:  100.00%\n",
      "Epoch: 42, Train Loss: 0.026, Train Acc: 98.43%, Val. Loss: 1.595243, Val. Acc: 73.84%\n",
      "Epoch: 43, Idx: 100, Training Loss: 0.0001, Training Accuracy:  100.00%\n",
      "Epoch: 43, Train Loss: 0.026, Train Acc: 98.59%, Val. Loss: 1.710958, Val. Acc: 73.56%\n",
      "Epoch: 44, Idx: 100, Training Loss: 0.0026, Training Accuracy:  100.00%\n",
      "Epoch: 44, Train Loss: 0.028, Train Acc: 98.51%, Val. Loss: 1.714843, Val. Acc: 72.56%\n",
      "Epoch: 45, Idx: 100, Training Loss: 0.0002, Training Accuracy:  100.00%\n",
      "Epoch: 45, Train Loss: 0.025, Train Acc: 98.51%, Val. Loss: 1.706748, Val. Acc: 73.75%\n",
      "Epoch: 46, Idx: 100, Training Loss: 0.0000, Training Accuracy:  100.00%\n",
      "Epoch: 46, Train Loss: 0.019, Train Acc: 98.81%, Val. Loss: 1.762538, Val. Acc: 73.56%\n",
      "Epoch: 47, Idx: 100, Training Loss: 0.0003, Training Accuracy:  100.00%\n",
      "Epoch: 47, Train Loss: 0.046, Train Acc: 98.25%, Val. Loss: 1.898619, Val. Acc: 73.03%\n",
      "Epoch: 48, Idx: 100, Training Loss: 0.0002, Training Accuracy:  100.00%\n",
      "Epoch: 48, Train Loss: 0.028, Train Acc: 98.45%, Val. Loss: 1.703879, Val. Acc: 74.60%\n",
      "Epoch: 49, Idx: 100, Training Loss: 0.0026, Training Accuracy:  100.00%\n",
      "Epoch: 49, Train Loss: 0.022, Train Acc: 98.69%, Val. Loss: 1.779756, Val. Acc: 74.48%\n",
      "Epoch: 50, Idx: 100, Training Loss: 0.0003, Training Accuracy:  100.00%\n",
      "Epoch: 50, Train Loss: 0.018, Train Acc: 98.69%, Val. Loss: 1.733330, Val. Acc: 74.19%\n",
      "Epoch: 51, Idx: 100, Training Loss: 0.0042, Training Accuracy:  100.00%\n",
      "Epoch: 51, Train Loss: 0.026, Train Acc: 98.57%, Val. Loss: 1.919831, Val. Acc: 73.71%\n",
      "Epoch: 52, Idx: 100, Training Loss: 0.0003, Training Accuracy:  100.00%\n",
      "Epoch: 52, Train Loss: 0.024, Train Acc: 98.55%, Val. Loss: 1.812363, Val. Acc: 72.83%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 53, Idx: 100, Training Loss: 0.0003, Training Accuracy:  100.00%\n",
      "Epoch: 53, Train Loss: 0.025, Train Acc: 98.63%, Val. Loss: 1.879748, Val. Acc: 72.72%\n",
      "Epoch: 54, Idx: 100, Training Loss: 0.1264, Training Accuracy:  96.88%\n",
      "Epoch: 54, Train Loss: 0.023, Train Acc: 98.59%, Val. Loss: 1.787848, Val. Acc: 74.18%\n",
      "Epoch: 55, Idx: 100, Training Loss: 0.0030, Training Accuracy:  100.00%\n",
      "Epoch: 55, Train Loss: 0.028, Train Acc: 98.49%, Val. Loss: 1.899788, Val. Acc: 73.02%\n",
      "Epoch: 56, Idx: 100, Training Loss: 0.0001, Training Accuracy:  100.00%\n",
      "Epoch: 56, Train Loss: 0.015, Train Acc: 98.77%, Val. Loss: 1.823974, Val. Acc: 74.41%\n",
      "Epoch: 57, Idx: 100, Training Loss: 0.0616, Training Accuracy:  96.88%\n",
      "Epoch: 57, Train Loss: 0.025, Train Acc: 98.61%, Val. Loss: 1.874429, Val. Acc: 73.92%\n",
      "Epoch: 58, Idx: 100, Training Loss: 0.0001, Training Accuracy:  100.00%\n",
      "Epoch: 58, Train Loss: 0.019, Train Acc: 98.73%, Val. Loss: 1.836038, Val. Acc: 73.96%\n",
      "Epoch: 59, Idx: 100, Training Loss: 0.1257, Training Accuracy:  93.75%\n",
      "Epoch: 59, Train Loss: 0.027, Train Acc: 98.45%, Val. Loss: 1.864804, Val. Acc: 73.62%\n",
      "Epoch: 60, Idx: 100, Training Loss: 0.0003, Training Accuracy:  100.00%\n",
      "Epoch: 60, Train Loss: 0.016, Train Acc: 98.85%, Val. Loss: 1.817449, Val. Acc: 73.54%\n",
      "Epoch: 61, Idx: 100, Training Loss: 0.0072, Training Accuracy:  100.00%\n",
      "Epoch: 61, Train Loss: 0.027, Train Acc: 98.37%, Val. Loss: 1.766422, Val. Acc: 74.19%\n",
      "Epoch: 62, Idx: 100, Training Loss: 0.0003, Training Accuracy:  100.00%\n",
      "Epoch: 62, Train Loss: 0.019, Train Acc: 98.87%, Val. Loss: 1.888669, Val. Acc: 73.97%\n",
      "Epoch: 63, Idx: 100, Training Loss: 0.0089, Training Accuracy:  100.00%\n",
      "Epoch: 63, Train Loss: 0.025, Train Acc: 98.69%, Val. Loss: 1.834538, Val. Acc: 73.13%\n",
      "Epoch: 64, Idx: 100, Training Loss: 0.0048, Training Accuracy:  100.00%\n",
      "Epoch: 64, Train Loss: 0.024, Train Acc: 98.57%, Val. Loss: 1.852542, Val. Acc: 73.06%\n",
      "Epoch: 65, Idx: 100, Training Loss: 0.0001, Training Accuracy:  100.00%\n",
      "Epoch: 65, Train Loss: 0.024, Train Acc: 98.61%, Val. Loss: 1.886849, Val. Acc: 73.86%\n",
      "Epoch: 66, Idx: 100, Training Loss: 0.0083, Training Accuracy:  100.00%\n",
      "Epoch: 66, Train Loss: 0.023, Train Acc: 98.63%, Val. Loss: 1.910876, Val. Acc: 73.38%\n",
      "Epoch: 67, Idx: 100, Training Loss: 0.0318, Training Accuracy:  96.88%\n",
      "Epoch: 67, Train Loss: 0.022, Train Acc: 98.69%, Val. Loss: 1.964726, Val. Acc: 73.09%\n",
      "Epoch: 68, Idx: 100, Training Loss: 0.0007, Training Accuracy:  100.00%\n",
      "Epoch: 68, Train Loss: 0.020, Train Acc: 98.59%, Val. Loss: 1.860506, Val. Acc: 74.22%\n",
      "Epoch: 69, Idx: 100, Training Loss: 0.0056, Training Accuracy:  100.00%\n",
      "Epoch: 69, Train Loss: 0.022, Train Acc: 98.83%, Val. Loss: 1.923771, Val. Acc: 72.92%\n",
      "Epoch: 70, Idx: 100, Training Loss: 0.0013, Training Accuracy:  100.00%\n",
      "Epoch: 70, Train Loss: 0.014, Train Acc: 98.83%, Val. Loss: 1.978858, Val. Acc: 73.94%\n",
      "Epoch: 71, Idx: 100, Training Loss: 0.0008, Training Accuracy:  100.00%\n",
      "Epoch: 71, Train Loss: 0.020, Train Acc: 98.77%, Val. Loss: 1.885305, Val. Acc: 74.03%\n",
      "Epoch: 72, Idx: 100, Training Loss: 0.0031, Training Accuracy:  100.00%\n",
      "Epoch: 72, Train Loss: 0.021, Train Acc: 98.75%, Val. Loss: 1.888623, Val. Acc: 73.30%\n",
      "Epoch: 73, Idx: 100, Training Loss: 0.0000, Training Accuracy:  100.00%\n",
      "Epoch: 73, Train Loss: 0.011, Train Acc: 98.89%, Val. Loss: 1.980386, Val. Acc: 74.09%\n",
      "Epoch: 74, Idx: 100, Training Loss: 0.0000, Training Accuracy:  100.00%\n",
      "Epoch: 74, Train Loss: 0.014, Train Acc: 98.87%, Val. Loss: 2.009327, Val. Acc: 74.00%\n",
      "Epoch: 75, Idx: 100, Training Loss: 0.0049, Training Accuracy:  100.00%\n",
      "Epoch: 75, Train Loss: 0.023, Train Acc: 98.67%, Val. Loss: 1.887671, Val. Acc: 73.56%\n",
      "Epoch: 76, Idx: 100, Training Loss: 0.0035, Training Accuracy:  100.00%\n",
      "Epoch: 76, Train Loss: 0.015, Train Acc: 98.77%, Val. Loss: 2.065414, Val. Acc: 73.73%\n",
      "Epoch: 77, Idx: 100, Training Loss: 0.0295, Training Accuracy:  96.88%\n",
      "Epoch: 77, Train Loss: 0.017, Train Acc: 98.77%, Val. Loss: 2.092786, Val. Acc: 74.42%\n",
      "Epoch: 78, Idx: 100, Training Loss: 0.0030, Training Accuracy:  100.00%\n",
      "Epoch: 78, Train Loss: 0.013, Train Acc: 98.87%, Val. Loss: 2.090419, Val. Acc: 73.48%\n",
      "Epoch: 79, Idx: 100, Training Loss: 0.0000, Training Accuracy:  100.00%\n",
      "Epoch: 79, Train Loss: 0.027, Train Acc: 98.49%, Val. Loss: 1.994914, Val. Acc: 73.89%\n",
      "Epoch: 80, Idx: 100, Training Loss: 0.0001, Training Accuracy:  100.00%\n",
      "Epoch: 80, Train Loss: 0.016, Train Acc: 98.83%, Val. Loss: 2.032156, Val. Acc: 73.16%\n",
      "Epoch: 81, Idx: 100, Training Loss: 0.0148, Training Accuracy:  100.00%\n",
      "Epoch: 81, Train Loss: 0.015, Train Acc: 98.85%, Val. Loss: 2.029828, Val. Acc: 73.04%\n",
      "Epoch: 82, Idx: 100, Training Loss: 0.1555, Training Accuracy:  87.50%\n",
      "Epoch: 82, Train Loss: 0.017, Train Acc: 98.77%, Val. Loss: 1.938784, Val. Acc: 73.43%\n",
      "Epoch: 83, Idx: 100, Training Loss: 0.0002, Training Accuracy:  100.00%\n",
      "Epoch: 83, Train Loss: 0.013, Train Acc: 98.93%, Val. Loss: 1.964730, Val. Acc: 74.08%\n",
      "Epoch: 84, Idx: 100, Training Loss: 0.0001, Training Accuracy:  100.00%\n",
      "Epoch: 84, Train Loss: 0.012, Train Acc: 98.91%, Val. Loss: 1.997864, Val. Acc: 73.94%\n",
      "Epoch: 85, Idx: 100, Training Loss: 0.0001, Training Accuracy:  100.00%\n",
      "Epoch: 85, Train Loss: 0.017, Train Acc: 98.83%, Val. Loss: 1.970370, Val. Acc: 73.64%\n",
      "Epoch: 86, Idx: 100, Training Loss: 0.0000, Training Accuracy:  100.00%\n",
      "Epoch: 86, Train Loss: 0.015, Train Acc: 98.77%, Val. Loss: 2.020456, Val. Acc: 73.76%\n",
      "Epoch: 87, Idx: 100, Training Loss: 0.0002, Training Accuracy:  100.00%\n",
      "Epoch: 87, Train Loss: 0.012, Train Acc: 98.95%, Val. Loss: 2.022689, Val. Acc: 74.23%\n",
      "Epoch: 88, Idx: 100, Training Loss: 0.0008, Training Accuracy:  100.00%\n",
      "Epoch: 88, Train Loss: 0.011, Train Acc: 98.89%, Val. Loss: 2.108312, Val. Acc: 73.27%\n",
      "Epoch: 89, Idx: 100, Training Loss: 0.0000, Training Accuracy:  100.00%\n",
      "Epoch: 89, Train Loss: 0.015, Train Acc: 98.83%, Val. Loss: 2.188563, Val. Acc: 72.71%\n",
      "Epoch: 90, Idx: 100, Training Loss: 0.0061, Training Accuracy:  100.00%\n",
      "Epoch: 90, Train Loss: 0.014, Train Acc: 98.87%, Val. Loss: 2.051399, Val. Acc: 73.98%\n",
      "Epoch: 91, Idx: 100, Training Loss: 0.0000, Training Accuracy:  100.00%\n",
      "Epoch: 91, Train Loss: 0.013, Train Acc: 98.95%, Val. Loss: 2.041519, Val. Acc: 73.78%\n",
      "Epoch: 92, Idx: 100, Training Loss: 0.0007, Training Accuracy:  100.00%\n",
      "Epoch: 92, Train Loss: 0.013, Train Acc: 98.85%, Val. Loss: 2.030241, Val. Acc: 73.89%\n",
      "Epoch: 93, Idx: 100, Training Loss: 0.0000, Training Accuracy:  100.00%\n",
      "Epoch: 93, Train Loss: 0.016, Train Acc: 98.89%, Val. Loss: 1.956204, Val. Acc: 74.65%\n",
      "Epoch: 94, Idx: 100, Training Loss: 0.0003, Training Accuracy:  100.00%\n",
      "Epoch: 94, Train Loss: 0.012, Train Acc: 98.97%, Val. Loss: 2.115845, Val. Acc: 73.90%\n",
      "Epoch: 95, Idx: 100, Training Loss: 0.1846, Training Accuracy:  96.88%\n",
      "Epoch: 95, Train Loss: 0.011, Train Acc: 98.97%, Val. Loss: 2.176008, Val. Acc: 74.05%\n",
      "Epoch: 96, Idx: 100, Training Loss: 0.0000, Training Accuracy:  100.00%\n",
      "Epoch: 96, Train Loss: 0.012, Train Acc: 98.97%, Val. Loss: 2.210260, Val. Acc: 74.17%\n",
      "Epoch: 97, Idx: 100, Training Loss: 0.0616, Training Accuracy:  96.88%\n",
      "Epoch: 97, Train Loss: 0.013, Train Acc: 98.85%, Val. Loss: 2.202591, Val. Acc: 73.72%\n",
      "Epoch: 98, Idx: 100, Training Loss: 0.0001, Training Accuracy:  100.00%\n",
      "Epoch: 98, Train Loss: 0.025, Train Acc: 98.69%, Val. Loss: 1.986965, Val. Acc: 73.63%\n",
      "Epoch: 99, Idx: 100, Training Loss: 0.0550, Training Accuracy:  96.88%\n",
      "Epoch: 99, Train Loss: 0.012, Train Acc: 98.95%, Val. Loss: 1.909338, Val. Acc: 73.15%\n",
      "Epoch: 100, Idx: 100, Training Loss: 0.0003, Training Accuracy:  100.00%\n",
      "Epoch: 100, Train Loss: 0.013, Train Acc: 98.93%, Val. Loss: 2.264552, Val. Acc: 73.50%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    train_loss, train_acc = train_model(model, train_iterator, epoch)\n",
    "    val_loss, val_acc = eval_model(model, valid_iterator)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%')\n",
    "    \n",
    "#test_loss, test_acc = eval_model(model, test_iter)\n",
    "#print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autor: Nietzsche\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea/miniconda3/envs/torchenv/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "test_sen1 = \"Thou art not stone but already hast thou become hollow by the numerous drops\"\n",
    "test_sen2 = \"Her thin fingers tear at the jewel to no purpose.\" #Wilde\n",
    "Text_nie = \"The great disgust at man IT strangled me and had crept into my throat and what the soothsayer had presaged all is alike nothing is worth while knowledge strangleth\" #Nietzsche\n",
    "\n",
    "test_sen1 = TEXT.preprocess(test_sen1)\n",
    "test_sen1 = [[TEXT.vocab.stoi[x] for x in test_sen1]]\n",
    "\n",
    "test_sen2 = TEXT.preprocess(Text_nie)\n",
    "test_sen2 = [[TEXT.vocab.stoi[x] for x in test_sen2]]\n",
    "\n",
    "test_sen = np.asarray(test_sen2)\n",
    "test_sen = torch.LongTensor(test_sen)\n",
    "test_tensor = Variable(test_sen, volatile=True)\n",
    "test_tensor = test_tensor.cuda()\n",
    "model.eval()\n",
    "output = model(test_tensor, 1)\n",
    "out = F.softmax(output, 1)\n",
    "if (torch.argmax(out[0]) == 0):\n",
    "    print (\"Autor: Hemingway\")\n",
    "elif(torch.argmax(out[0]) == 1):\n",
    "    print (\"Autor: Nietzsche\")\n",
    "elif(torch.argmax(out[0]) == 2):\n",
    "    print (\"Autor: Wilde\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "torchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
